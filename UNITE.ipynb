{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbmeSoy6C9Ux"
      },
      "source": [
        "데이터셋은 우선적으로는 CelebDFV2를 사용하고, 이후에 GTA-V 데이터를 합쳐보도록 하자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpBO_bgZCCur"
      },
      "source": [
        "이걸 어디부터 구현해야 할까...\n",
        "\n",
        "일단 SigLIP을 불러오는 것부터 시작하자. Quantize를 해줘서 최대한 부담을 줄여주자."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요한 라이브러리 설치 & 초기화"
      ],
      "metadata": {
        "id": "p8J6322mUvLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NPK-Ap0dOit",
        "outputId": "856a2795-8d36-4134-e060-9881c61f0790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 98ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --system flash-attn --no-build-isolation\n",
        "!pip install -q flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvXKooN5CCOG",
        "outputId": "9a44af62-858a-42b8-c38e-7908d4abf9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 98ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --system lightning\n",
        "!pip install -q lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le5t6iy32IYI",
        "outputId": "3f6a54f7-e01e-43d3-928a-7b3f30cb4177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            " V..... av1_cuvid            Nvidia CUVID AV1 decoder (codec av1)\n",
            " V..... h264_cuvid           Nvidia CUVID H264 decoder (codec h264)\n",
            " V..... hevc_cuvid           Nvidia CUVID HEVC decoder (codec hevc)\n",
            " V..... mjpeg_cuvid          Nvidia CUVID MJPEG decoder (codec mjpeg)\n",
            " V..... mpeg1_cuvid          Nvidia CUVID MPEG1VIDEO decoder (codec mpeg1video)\n",
            " V..... mpeg2_cuvid          Nvidia CUVID MPEG2VIDEO decoder (codec mpeg2video)\n",
            " V..... mpeg4_cuvid          Nvidia CUVID MPEG4 decoder (codec mpeg4)\n",
            " V..... vc1_cuvid            Nvidia CUVID VC1 decoder (codec vc1)\n",
            " V..... vp8_cuvid            Nvidia CUVID VP8 decoder (codec vp8)\n",
            " V..... vp9_cuvid            Nvidia CUVID VP9 decoder (codec vp9)\n"
          ]
        }
      ],
      "source": [
        "!ffmpeg -decoders | grep -i nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5NX_61lI4WV",
        "outputId": "5bf19dc5-07b8-4e2d-b2f7-cbed03fec015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 96ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --system bitsandbytes\n",
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PVC-h-3x9aO",
        "outputId": "4d852c30-4c1d-4e93-b480-9893920847f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 370ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 0.33ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtorchcodec\u001b[0m\u001b[2m==0.9.0+cu126\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --force-reinstall torchcodec==0.9.0 --index-url=https://download.pytorch.org/whl/cu126"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "39WGLUK1Irpp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# 컴파일 워커를 2개 정도로 제한 (전체 12개 중 일부만 사용)\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"2\"\n",
        "# os.environ[\"TORCH_COMPILE_DEBUG\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnPg2QCWc-Ln",
        "outputId": "a8ff413b-f84c-4281-a3b3-79c73f7ee01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CesM4HbVBZWY"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as L\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1LPodHaYK4c"
      },
      "source": [
        "embed_size = 768\n",
        "\n",
        "frame_token = 576"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZwnOwZsXtZR"
      },
      "source": [
        "솔직히, 왠만하면 원본 모델을 따라가고 싶지만... 일단 인코더를 더 경량으로 바꾼다.\n",
        "\n",
        "나중에 성능 안 나오면 탓할 것 중 인코더가 늘었다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cTmv1qBoXnHs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 구현"
      ],
      "metadata": {
        "id": "gEbynKYqU10T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인코더"
      ],
      "metadata": {
        "id": "Q3Ztq0iMU3a9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0p36S4LXYIzn"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from flash_attn import flash_attn_func\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "        assert self.head_dim * num_heads == embed_size, \"embed_size must be divisible by num_heads\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.k_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.v_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.dropout_p = dropout\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x, return_attn_output=False):\n",
        "        # x: (batch, token/frame * frames, embed_size)\n",
        "        batch_size, frame_token, embed_size = x.shape\n",
        "\n",
        "        # Project queries, keys, values\n",
        "        q = self.q_proj(x) # (batch, token/frame * frames, embed_size)\n",
        "        k = self.k_proj(x) # (batch, token/frame * frames, embed_size)\n",
        "        v = self.v_proj(x) # (batch, token/frame * frames, embed_size)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        q = q.view(batch_size, frame_token, self.num_heads, self.head_dim) # (batch, token/frame * frames, num_heads, head_dim)\n",
        "        k = k.view(batch_size, frame_token, self.num_heads, self.head_dim) # (batch, token/frame * frames, num_heads, head_dim)\n",
        "        v = v.view(batch_size, frame_token, self.num_heads, self.head_dim) # (batch, token/frame * frames, num_heads, head_dim)\n",
        "\n",
        "        # Apply scaled dot product attention\n",
        "        # dropout_p는 훈련 중일 때만 적용\n",
        "        attn_output_raw = flash_attn_func(\n",
        "            q, k, v,\n",
        "            dropout_p=self.dropout_p if self.training else 0.0,\n",
        "            softmax_scale=None, # None이면 1/sqrt(head_dim) 자동 적용\n",
        "            causal=False\n",
        "        )\n",
        "        # attn_output: (batch, token/frame * frames, num_heads, head_dim)\n",
        "\n",
        "        # Concatenate heads and apply final linear projection\n",
        "        attn_output = attn_output_raw.contiguous().view(batch_size, frame_token, embed_size)\n",
        "        attn_output = self.out_proj(attn_output) # (batch_size, seq_len, embed_size)\n",
        "\n",
        "\n",
        "        x = self.ln1(x + attn_output) # Residual connection + LayerNorm\n",
        "        x = self.ln2(x + self.mlp(x))\n",
        "\n",
        "        if return_attn_output:\n",
        "            return x, attn_output_raw\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "KlCadkfgU6Eq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IA7_g3dQb3rI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class TemporalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 32, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Feature dimension (ds = 768)\n",
        "            max_len: Maximum number of frames (nf = 32)\n",
        "            dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # 1. Frame Index (j) 생성: 0 ~ max_len-1\n",
        "        # 논문의 'j'에 해당합니다.\n",
        "        position = torch.arange(max_len).unsqueeze(1).float()  # [max_len, 1]\n",
        "\n",
        "        # 2. Div Term 계산 (논문 Eq 1의 분모 부분)\n",
        "        # 10000^(2i/ds) 부분을 로그 스케일로 계산\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # 3. PE Matrix 초기화 [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # 4. 논문 수식 (Eq 1) 적용\n",
        "        # PE(j, 2i) = sin(...) -> 짝수 인덱스\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # PE(j, 2i+1) = cos(...) -> 홀수 인덱스\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # 5. 차원 확장 (Broadcasting 준비)\n",
        "        # PE는 [Frame, Dim] 정보를 담고 있습니다.\n",
        "        # 입력 x: [Batch, Frames, Tokens, Dim]\n",
        "        # PE  : [1,     Frames, 1,      Dim] 형태로 만들어야\n",
        "        # Batch와 Tokens 차원으로 자동 확장(Broadcast)되어 더해집니다.\n",
        "        pe = pe.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [Batch, Frames, Tokens, Dim]\n",
        "               (예: [B, 32, 576, 768])\n",
        "        Returns:\n",
        "            x: Temporal PE가 더해진 텐서\n",
        "        \"\"\"\n",
        "        # 입력된 비디오의 실제 프레임 수만큼만 PE를 잘라서 사용\n",
        "        current_frames = x.size(1)\n",
        "\n",
        "        # x에 PE를 더함.\n",
        "        # pe[:, :current_frames, :, :]의 shape은 [1, F, 1, D]\n",
        "        # x의 shape [B, F, T, D]에 맞춰서,\n",
        "        # 같은 프레임(F)에 있는 모든 토큰(T)들에게 동일한 PE 벡터가 더해짐.\n",
        "        x = x + self.pe[:, :current_frames, :, :]\n",
        "\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Preprocessor\n",
        "당장은 안 쓰이지만, 혹시나 필요하다면"
      ],
      "metadata": {
        "id": "wlsC7yiGU9Dh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fx_U3LxUoMqM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "class GPUSigLIPProcessor:\n",
        "    def __init__(self, processor):\n",
        "        config = processor.image_processor\n",
        "\n",
        "        # 1. 리사이즈 설정: Bilinear + Antialias=True가 핵심\n",
        "        # Fast 프로세서가 텐서를 처리할 때 사용하는 로직과 일치시킵니다.\n",
        "        self.resize = v2.Resize(\n",
        "            size=(config.size['height'], config.size['width']),\n",
        "            interpolation=v2.InterpolationMode.BILINEAR, # resample=2\n",
        "            antialias=True # 오차를 줄이는 가장 중요한 설정\n",
        "        )\n",
        "\n",
        "        # 2. 정규화 설정\n",
        "        # (x - 0.5) / 0.5 연산\n",
        "        self.mean = torch.tensor(config.image_mean).view(1, 3, 1, 1)\n",
        "        self.std = torch.tensor(config.image_std).view(1, 3, 1, 1)\n",
        "        self.rescale_factor = config.rescale_factor\n",
        "\n",
        "    def __call__(self, video_tensor):\n",
        "        \"\"\"\n",
        "        video_tensor: (B, 3, T, H, W), uint8, GPU\n",
        "        \"\"\"\n",
        "        b, c, t, h, w = video_tensor.shape\n",
        "        device = video_tensor.device\n",
        "\n",
        "        # 차원 변경 (B*T, C, H, W)\n",
        "        x = video_tensor.permute(0, 2, 1, 3, 4)\n",
        "        x = x.flatten(0, 1)\n",
        "\n",
        "        # [Step 1] Resize (uint8 상태에서 수행하거나 float32에서 수행)\n",
        "        # torchvision v2는 uint8 입력을 받아 내부적으로 고정밀 연산을 수행합니다.\n",
        "        x = self.resize(x)\n",
        "\n",
        "        # [Step 2] Float32 변환 및 Rescale (0~255 -> 0~1)\n",
        "        x = x.to(torch.float32) * self.rescale_factor\n",
        "\n",
        "        # [Step 3] Normalize (x - 0.5) / 0.5\n",
        "        # mean, std를 캐싱하여 속도 최적화\n",
        "        self.mean = self.mean.to(device)\n",
        "        self.std = self.std.to(device)\n",
        "        x = (x - self.mean) / self.std\n",
        "\n",
        "        # [Step 4] 최종 모델 입력형태인 float16으로 반환\n",
        "        return x.to(torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNITE"
      ],
      "metadata": {
        "id": "HUor9lskVEW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dl5XX6bXbf46"
      },
      "outputs": [],
      "source": [
        "class UNITE(nn.Module):\n",
        "    def __init__(\n",
        "            self, num_channel=3, num_cls=2, num_heads=12, max_len=32, dropout=0.1,\n",
        "            encoder_model=\"google/siglip2-base-patch16-384\", use_bfloat=True, cpu_preprocess=True\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        dtype = torch.bfloat16 if use_bfloat else torch.float16\n",
        "        self.vis_encoder = AutoModel.from_pretrained(\n",
        "            encoder_model,\n",
        "            device_map=\"auto\",\n",
        "            dtype=dtype,\n",
        "            attn_implementation=\"flash_attention_2\",\n",
        "        )\n",
        "        self.embed_size = self.vis_encoder.config.vision_config.hidden_size\n",
        "        processor = AutoProcessor.from_pretrained(encoder_model, use_fast=True)\n",
        "        self.processor = GPUSigLIPProcessor(processor)\n",
        "        self.num_heads = num_heads\n",
        "        self.cpu_preprocess = cpu_preprocess\n",
        "\n",
        "        for para in self.vis_encoder.parameters():\n",
        "            para.requires_grad = False\n",
        "        self.vis_encoder.eval()\n",
        "\n",
        "        self.class_token = nn.Parameter(torch.randn((self.embed_size,)), requires_grad=True)\n",
        "\n",
        "        self.pos_embedding = TemporalPositionalEncoding(self.embed_size, max_len)\n",
        "        self.first_encoder = ViTEncoder(self.embed_size, num_heads, dropout)\n",
        "        self.encoders = nn.ModuleList([ViTEncoder(self.embed_size, num_heads, dropout) for _ in range(3)])\n",
        "        self.mlp_head = nn.Linear(self.embed_size, num_cls)\n",
        "\n",
        "\n",
        "    def forward(self, x, return_ad_param=False, return_embed=False):\n",
        "        self.vis_encoder.eval()\n",
        "\n",
        "        # Input: [batch, c, frame, h, w]\n",
        "        b, _, f, *_ = x.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.cpu_preprocess:\n",
        "                x = x.permute(0, 2, 1, 3, 4).flatten(0, 1) # (B, C, F, H, W) -> (B*F, C, H, W)\n",
        "                x = x.to(self.vis_encoder.dtype)\n",
        "            else:\n",
        "                # -> Preprocessing [batch * frame, c, h, w]\n",
        "                x = self.processor(x)\n",
        "            # -> Visual encoding [batch * frame, token/frame(576), dim/token (embed_size)]\n",
        "            x = self.vis_encoder.vision_model(pixel_values=x).last_hidden_state\n",
        "        # -> [batch, frame, token/frame, dim/token]\n",
        "        x = x.reshape(b, f, -1, self.embed_size)\n",
        "        x = self.pos_embedding(x)\n",
        "        train_in = x # xi: [batch, frame, token/frame, dim/token]\n",
        "\n",
        "        _, _, t, d = x.shape\n",
        "        # Reshape for transformer\n",
        "        # -> [batch, token/frame * frame, dim/token]\n",
        "        x = x.reshape(b, t*f, d)\n",
        "        # Add class token [batch, 1, dim/token]\n",
        "        cls_token = self.class_token.view(1, 1, -1).expand(b, -1, -1)\n",
        "        # -> [batch, (token/frame) * frame + 1, dim/token]\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        P = None\n",
        "        if return_ad_param:\n",
        "            # attn_output: [batch, (token/frame) * frame + 1, num_head, (dim/token) / num_head]\n",
        "            # x: [batch, (token/frame) * frame + 1, dim/token]\n",
        "            x, attn_output = self.first_encoder(x, return_attn_output=True)\n",
        "            # -> [batch, num_head, (token/frame) * frame, (dim/token) / num_head]\n",
        "            attn_output = attn_output.permute(0, 2, 1, 3)[:, :, 1:, :]\n",
        "            # xi -> [batch, (token/frame) * frame, num_head, (dim/token) / num_head] -> [batch, num_head, (token/frame) * frame, (dim/token) / num_head]\n",
        "            train_in = train_in.reshape(b, -1, self.num_heads, d // self.num_heads).permute(0, 2, 1, 3)\n",
        "            P = attn_output * train_in\n",
        "            # -> [batch, num_head, token/frame, frame, (dim/token) / num_head]\n",
        "            # -> [batch, num_head, frame, token/frame, (dim/token) / num_head]\n",
        "            P = P.reshape(b, self.num_heads, t, f, -1).permute(0, 1, 3, 2, 4)\n",
        "            # -> [batch, num_head, frame]\n",
        "            P = P.sum(dim=(3, 4))\n",
        "        else:\n",
        "            x = self.first_encoder(x)\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "\n",
        "        # Get only cls_token\n",
        "        # [batch, 1, dim/token]\n",
        "        x = x[:, 0, :]\n",
        "        # [batch, dim/token]\n",
        "        x = x.reshape(b, -1)\n",
        "        embed = x\n",
        "        x = self.mlp_head(x)\n",
        "        res = [x]\n",
        "        if return_ad_param:\n",
        "            res.append(P)\n",
        "        if return_embed:\n",
        "            res.append(embed)\n",
        "        return tuple(res) if len(res) > 1 else res[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AD Loss"
      ],
      "metadata": {
        "id": "iM_x3Oc5VGLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OflIU8asnbGT"
      },
      "outputs": [],
      "source": [
        "class ADLoss(nn.Module):\n",
        "    def __init__(self, num_cls=2, num_heads=12, max_len=32, delta_within=(0.01, -2.0), delta_between=0.5, eta=0.05):\n",
        "        super().__init__()\n",
        "        # C shape: [num_heads, max_len]\n",
        "        C = torch.zeros(num_heads, max_len)\n",
        "        self.register_buffer('C', C)\n",
        "\n",
        "        self.num_cls = num_cls\n",
        "        self.num_heads = num_heads\n",
        "        self.delta_within = torch.tensor(delta_within) # [0.01, -2.0] (True, Fake)\n",
        "        self.delta_between = delta_between # 0.5\n",
        "        self.eta = eta\n",
        "\n",
        "    def forward(self, P, labels, log_detail=False):\n",
        "        \"\"\"\n",
        "        P: [batch, num_heads, max_len]\n",
        "        labels: [batch] (Class indices)\n",
        "        \"\"\"\n",
        "        device = P.device\n",
        "        # 각 헤드(H)별로 정규화해야 논문의 '헤드별 센터' 개념에 맞습니다.\n",
        "        P_norm = F.normalize(P, p=2, dim=-1) # 각 헤드의 특징 벡터(F)를 정규화\n",
        "\n",
        "        # 1. 센터 업데이트 (현재 로직 유지하되 헤드별로 정규화 상태 유지)\n",
        "        if self.training:\n",
        "            self.C = (1 - self.eta) * self.C + self.eta * P_norm.mean(dim=0).detach()\n",
        "\n",
        "        # 센터 정규화 (거리 계산 전 필수)\n",
        "        C_norm = F.normalize(self.C, p=2, dim=-1) # [H, F]\n",
        "\n",
        "        # --- 2. Within-class Loss (식 4) ---\n",
        "        # 각 샘플과 전체 센터 사이의 거리\n",
        "        # P: [B, H, F], self.C: [H, F]\n",
        "        diff_within = P_norm - C_norm\n",
        "        # L2 Norm 계산 (헤드와 프레임 차원에 대해)\n",
        "        dist_within = torch.norm(diff_within, p=2, dim=(-1, -2))\n",
        "\n",
        "        # 각 클래스별로 다른 delta 적용\n",
        "        delta_per_batch = self.delta_within[labels]\n",
        "        loss_within = torch.relu(dist_within - delta_per_batch).mean()\n",
        "\n",
        "\n",
        "        # 3. Between-class Loss -> 사실상 \"Between-Head Diversity Loss\"\n",
        "        # 논문 식 (5)의 (k, l) in (nh, nh)를 구현\n",
        "\n",
        "        C_mean = F.normalize(self.C.sum(dim=0), p=2, dim=-1)\n",
        "\n",
        "        # 헤드 간 Pairwise 거리: [H, H]\n",
        "        dist_matrix = torch.cdist(C_mean, C_mean, p=2)\n",
        "\n",
        "        # k != l 인 상삼각 행렬 마스크\n",
        "        mask = torch.triu(torch.ones(self.num_heads, self.num_heads, device=device), diagonal=1).bool()\n",
        "        different_heads_dist = dist_matrix[mask]\n",
        "\n",
        "        # 헤드들이 최소 delta_between만큼은 떨어져 있도록 함\n",
        "        loss_between = torch.relu(self.delta_between - different_heads_dist).mean()\n",
        "\n",
        "        if log_detail:\n",
        "            return loss_within + loss_between, loss_within, loss_between\n",
        "\n",
        "        return loss_within + loss_between"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 준비물"
      ],
      "metadata": {
        "id": "Y3au2unjVI8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w_3YGmU93VT8"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.classification import Accuracy, AveragePrecision, Precision, Recall, ROC, AUROC\n",
        "from torchmetrics import MetricCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Lightning 모듈"
      ],
      "metadata": {
        "id": "R1W6XRk1VLtq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mwpa-9_OuUpJ"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Literal, NotRequired\n",
        "from torchmetrics import ConfusionMatrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "class LitUNITEClassifier(L.LightningModule):\n",
        "    def __init__(\n",
        "            self, num_cls=2, num_heads=12, max_len=32, dropout=0.1,\n",
        "            encoder_model=\"google/siglip2-base-patch16-384\", use_bfloat=True, cpu_preprocess=True,\n",
        "            delta_within=(0.01, -2.0), delta_between=0.5, eta=0.05,\n",
        "            lambda_1=0.5, lambda_2=0.5, lr=1e-4, decay_steps=1000,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = UNITE(\n",
        "            num_cls=num_cls,\n",
        "            num_heads=num_heads,\n",
        "            max_len=max_len,\n",
        "            dropout=dropout,\n",
        "            encoder_model=encoder_model,\n",
        "            use_bfloat=use_bfloat,\n",
        "            cpu_preprocess=cpu_preprocess\n",
        "        )\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.ad_loss = ADLoss(\n",
        "            num_cls=num_cls,\n",
        "            num_heads=num_heads,\n",
        "            max_len=max_len,\n",
        "            delta_within=delta_within,\n",
        "            delta_between=delta_between,\n",
        "            eta=eta,\n",
        "        )\n",
        "        self.lambda_1 = lambda_1\n",
        "        self.lambda_2 = lambda_2\n",
        "\n",
        "        self.lr = lr\n",
        "        self.decay_steps = decay_steps # Set this in respect to batch size; original batch size was 32\n",
        "\n",
        "        MetricType = TypedDict('MetricType', {\n",
        "            \"task\": Literal['multiclass'] | Literal['binary'],\n",
        "            \"num_classes\": NotRequired[int],\n",
        "            \"average\": NotRequired[Literal['macro']],\n",
        "        })\n",
        "\n",
        "        metric_param: MetricType = {\"task\": \"multiclass\", \"num_classes\": num_cls, \"average\": \"macro\"}\n",
        "\n",
        "        metrics = MetricCollection([\n",
        "            Accuracy(**metric_param),\n",
        "            AveragePrecision(**metric_param),\n",
        "            Precision(**metric_param),\n",
        "            Recall(**metric_param),\n",
        "            AUROC(**metric_param),\n",
        "        ])\n",
        "\n",
        "        self.val_metrics = metrics.clone(prefix=\"val/\")\n",
        "        self.val_roc = ROC(**metric_param)\n",
        "        self.test_metrics = metrics.clone(prefix=\"test/\")\n",
        "        self.test_roc = ROC(**metric_param)\n",
        "\n",
        "        self.confmat = ConfusionMatrix(task='multiclass', num_classes=num_cls)\n",
        "        self.num_cls = num_cls\n",
        "\n",
        "        self.class_names = [\"Real\", \"Fake\"] if num_cls == 2 else [f\"Class {i}\" for i in range(num_cls)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        \"\"\"체크포인트 저장 시 Frozen 파라미터(Backbone)를 제외합니다.\"\"\"\n",
        "        state_dict = checkpoint['state_dict']\n",
        "\n",
        "        # 저장하지 않을 키 필터링 (vis_encoder 관련 키들 제거)\n",
        "        # 키 이름은 모델 구조에 따라 'model.vis_encoder.'로 시작합니다.\n",
        "        keys_to_remove = [k for k in state_dict.keys() if \"model.vis_encoder\" in k]\n",
        "\n",
        "        for k in keys_to_remove:\n",
        "            del state_dict[k]\n",
        "\n",
        "\n",
        "    def on_load_checkpoint(self, checkpoint):\n",
        "        \"\"\"로드 시 체크포인트에 없는 백본 가중치를 현재 모델에서 복사해서 채워줌\"\"\"\n",
        "        state_dict = checkpoint['state_dict']\n",
        "        model_state_dict = self.state_dict()\n",
        "\n",
        "        # 현재 모델(self.model.vis_encoder)은 이미 __init__에서\n",
        "        # from_pretrained로 로드된 상태입니다.\n",
        "        # 체크포인트에 없는 키(백본 가중치)들을 모델의 현재 가중치로 채워줍니다.\n",
        "        for key in model_state_dict:\n",
        "            if key not in state_dict:\n",
        "                state_dict[key] = model_state_dict[key]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit, P = self.model(x, return_ad_param=True)\n",
        "        loss_ad, within, between = self.ad_loss(P, y, log_detail=True)\n",
        "        loss_ce = self.ce_loss(logit, y)\n",
        "        loss = loss_ce * self.lambda_1 + loss_ad * self.lambda_2\n",
        "\n",
        "\n",
        "        self.log_dict({\n",
        "            \"train/loss_ad\": loss_ad,\n",
        "            \"train/loss_ad/loss_within\": within,\n",
        "            \"train/loss_ad/loss_between\": between,\n",
        "            \"train/loss_ce\": loss_ce,\n",
        "            \"train/loss\": loss,\n",
        "        }, logger=True)\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_start(self):\n",
        "        self.val_preds = []\n",
        "        self.val_labels = []\n",
        "        self.val_embeds = []\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit, P, embed = self.model(x, return_ad_param=True, return_embed=True)\n",
        "        loss_ad = self.ad_loss(P, y)\n",
        "        loss_ce = self.ce_loss(logit, y)\n",
        "        loss = loss_ce * self.lambda_1 + loss_ad * self.lambda_2\n",
        "        self.log(\"val/loss_ad\", loss_ad, logger=True)\n",
        "        self.log(\"val/loss_ce\", loss_ce, logger=True)\n",
        "        self.log(\"val/loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        self.val_metrics.update(logit, y)\n",
        "        self.log_dict(self.val_metrics, logger=True)\n",
        "\n",
        "\n",
        "        # 데이터 수집 (CPU로 이동하여 메모리 절약)\n",
        "        preds = torch.argmax(logit, dim=1)\n",
        "        self.val_roc.update(logit, y)\n",
        "\n",
        "\n",
        "        self.val_preds.append(preds.detach().cpu())\n",
        "        self.val_labels.append(y.detach().cpu())\n",
        "        self.val_embeds.append(embed.detach().cpu()) # [Batch, 768]\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # 모든 배치 데이터 병합\n",
        "        all_preds = torch.cat(self.val_preds)\n",
        "        all_labels = torch.cat(self.val_labels)\n",
        "        all_embeds = torch.cat(self.val_embeds)\n",
        "\n",
        "        # 1. Confusion Matrix 시각화 및 로그\n",
        "        self._log_confusion_matrix(all_preds, all_labels, stage=\"val\")\n",
        "\n",
        "        # 2. t-SNE 시각화 및 로그\n",
        "        self._log_tsne(all_embeds, all_labels, predicts=all_preds, stage=\"val\")\n",
        "\n",
        "        fig, ax = self.val_roc.plot()\n",
        "        if self.logger:\n",
        "            self.logger.experiment.log({\"val/ROC_curve\": wandb.Image(fig)})\n",
        "        self.val_roc.reset()\n",
        "        plt.close(fig)\n",
        "\n",
        "        # 메모리 해제\n",
        "        self.val_preds.clear()\n",
        "        self.val_labels.clear()\n",
        "        self.val_embeds.clear()\n",
        "\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "        self.test_preds = []\n",
        "        self.test_labels = []\n",
        "        self.test_embeds = []\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit, embed = self.model(x, return_embed=True)\n",
        "        preds = torch.argmax(logit, dim=1)\n",
        "\n",
        "        self.test_preds.append(preds.detach().cpu())\n",
        "        self.test_labels.append(y.detach().cpu())\n",
        "        self.test_embeds.append(embed.detach().cpu())\n",
        "\n",
        "        self.test_metrics.update(logit, y)\n",
        "        self.log_dict(self.test_metrics, logger=True)\n",
        "\n",
        "        self.test_roc.update(logit, y)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        all_preds = torch.cat(self.test_preds)\n",
        "        all_labels = torch.cat(self.test_labels)\n",
        "        all_embeds = torch.cat(self.test_embeds)\n",
        "\n",
        "        # 1. Confusion Matrix\n",
        "        self._log_confusion_matrix(all_preds, all_labels, stage=\"test\")\n",
        "\n",
        "        # 2. t-SNE\n",
        "        self._log_tsne(all_embeds, all_labels, predicts=all_preds, stage=\"test\")\n",
        "\n",
        "        fig, ax = self.test_roc.plot()\n",
        "        if self.logger:\n",
        "            self.logger.experiment.log({\"test/ROC_curve\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "        self.test_roc.reset()\n",
        "\n",
        "        self.test_preds.clear()\n",
        "        self.test_labels.clear()\n",
        "        self.test_embeds.clear()\n",
        "\n",
        "    def _log_confusion_matrix(self, preds, labels, stage=\"val\"):\n",
        "        \"\"\"Confusion Matrix를 그리고 WandB에 로그합니다.\"\"\"\n",
        "        conf_matrix = self.confmat(preds.to(self.device), labels.to(self.device))\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            conf_matrix.cpu().numpy(),\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.class_names,\n",
        "            yticklabels=self.class_names\n",
        "        )\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(f'{stage.capitalize()} Confusion Matrix')\n",
        "\n",
        "        # WandB 로그\n",
        "        if self.logger:\n",
        "            self.logger.experiment.log({f\"{stage}/confusion_matrix\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "\n",
        "    def _log_tsne(self, embeds, labels, predicts=None, stage=\"val\", max_samples=2000):\n",
        "        \"\"\"t-SNE를 계산하고 WandB에 로그합니다. 데이터가 많으면 샘플링합니다.\"\"\"\n",
        "\n",
        "        # 데이터가 너무 많으면 t-SNE 속도가 매우 느려지므로 샘플링\n",
        "        num_samples = embeds.shape[0]\n",
        "        if num_samples > max_samples:\n",
        "            indices = torch.randperm(num_samples)[:max_samples]\n",
        "            embeds = embeds[indices]\n",
        "            labels = labels[indices]\n",
        "            predicts = predicts[indices]\n",
        "\n",
        "        # 텐서를 numpy로 변환\n",
        "        X = embeds.numpy()\n",
        "        y = labels.numpy()\n",
        "\n",
        "        # t-SNE 계산\n",
        "        # perplexity는 데이터 수보다 작아야 함 (보통 30~50)\n",
        "        n_components = 2\n",
        "        perplexity = min(30, num_samples - 1)\n",
        "        if perplexity < 5: return # 데이터가 너무 적으면 패스\n",
        "\n",
        "        tsne = TSNE(n_components=n_components, perplexity=perplexity, max_iter=1000, random_state=42)\n",
        "        X_embedded = tsne.fit_transform(X)\n",
        "\n",
        "        # DataFrame 생성 (시각화 용이성 위해)\n",
        "        df_tsne = pd.DataFrame(X_embedded, columns=['x', 'y'])\n",
        "        df_tsne['label'] = [self.class_names[i] for i in y]\n",
        "\n",
        "        style_col = None\n",
        "        if predicts is not None:\n",
        "            y_pred = predicts\n",
        "            df_tsne['prediction'] = [self.class_names[i] for i in y_pred]\n",
        "            style_col = 'prediction'\n",
        "\n",
        "        # Plotting\n",
        "        fig = plt.figure(figsize=(15, 12))\n",
        "        sns.scatterplot(\n",
        "            data=df_tsne, x='x', y='y', hue='label',\n",
        "            style=style_col,\n",
        "            palette=['blue', 'red'], # Real: Blue, Fake: Red\n",
        "            markers={'Real': 'o', 'Fake': 'X'},\n",
        "            alpha=0.7, s=100\n",
        "        )\n",
        "        plt.title(f'{stage.capitalize()} t-SNE Visualization')\n",
        "        plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # WandB 로그\n",
        "        if self.logger:\n",
        "            self.logger.experiment.log({f\"{stage}/t_sne\": wandb.Image(fig)})\n",
        "        plt.close(fig)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optim, self.decay_steps, gamma=0.5)\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optim,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\",\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋"
      ],
      "metadata": {
        "id": "z5ZspzwPVQ9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HTUa5d_5Vd2b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import cv2\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Sequence\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, ConcatDataset\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import transformers\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DeepFakeBaseDataset(Dataset, ABC):\n",
        "    def __init__(\n",
        "            self,\n",
        "            paths: Sequence[Path | str],\n",
        "            length=32, size=(384, 384),\n",
        "            transform=None,\n",
        "            device='cpu',\n",
        "            encoder_model=\"google/siglip2-base-patch16-384\",\n",
        "            cpu_preprocess=False,\n",
        "        ):\n",
        "        self.length = length\n",
        "        self.size = size\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "        self.samples = []\n",
        "\n",
        "        print(f\"Processing {len(paths)} paths...\")\n",
        "        self._prepare_samples(paths)\n",
        "        print(f\"Loaded {len(self.samples)} samples from {len(paths)} files/directories.\")\n",
        "\n",
        "        self.preprocessor = None\n",
        "        self.cpu_preprocess = cpu_preprocess\n",
        "        if cpu_preprocess:\n",
        "            self.preprocessor = transformers.AutoProcessor.from_pretrained(\n",
        "                encoder_model,\n",
        "                use_fast=True\n",
        "            )\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_label(self, path: str) -> int | None:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _prepare_samples(self, paths: Sequence[Path | str]):\n",
        "        \"\"\"상속받는 클래스에서 각자의 방식으로 samples 리스트를 채움\"\"\"\n",
        "        for path in paths:\n",
        "            path_str = str(path)\n",
        "            label = self._get_label(path_str)\n",
        "            if label is None:\n",
        "                continue\n",
        "\n",
        "            frame_cnt = self._get_frame_count(path_str)\n",
        "            if frame_cnt <= 0:\n",
        "                continue\n",
        "\n",
        "            num_chunks = self._calculate_num_chunks(frame_cnt)\n",
        "            for i in range(num_chunks):\n",
        "                self.samples.append({\n",
        "                    \"path\": path_str,\n",
        "                    \"chunk_idx\": i,\n",
        "                    \"label\": label,\n",
        "                    \"total_frames\": frame_cnt\n",
        "                })\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_frame_count(self, path: str) -> int:\n",
        "        \"\"\"자식 클래스에서 구현: 전체 프레임 수 반환\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def _calculate_num_chunks(self, frame_cnt: int) -> int:\n",
        "        \"\"\"자식 클래스에서 구현: 프레임 수에 따른 청크 개수 계산\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_label_counter(self) -> Counter:\n",
        "        return Counter([x['label'] for x in self.samples])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    @abstractmethod\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"자식 클래스에서 구현: 실제 데이터 로드\"\"\"\n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Celeb-DF"
      ],
      "metadata": {
        "id": "Gcq6DJ_BXN-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mCig1aG-MBvI"
      },
      "outputs": [],
      "source": [
        "class CelebDFBaseDataset(DeepFakeBaseDataset, ABC):\n",
        "    \"\"\"\n",
        "    Celeb-DF 데이터셋의 공통 기능을 담은 부모 클래스\n",
        "    \"\"\"\n",
        "    def _get_label(self, path: str) -> int | None:\n",
        "        \"\"\"폴더명 기반 레이블 결정\"\"\"\n",
        "        rel_path = path.replace(\"\\\\\", \"/\")\n",
        "        if \"YouTube-real\" in rel_path or \"Celeb-real\" in rel_path:\n",
        "            return 0\n",
        "        elif \"Celeb-synthesis\" in rel_path:\n",
        "            return 1\n",
        "        return None\n",
        "\n",
        "class CelebDFVideoDataset(CelebDFBaseDataset):\n",
        "    \"\"\"\n",
        "    비디오 파일(.mp4 등)에서 직접 프레임을 추출하는 데이터셋\n",
        "    \"\"\"\n",
        "    def _get_frame_count(self, path: str) -> int:\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        cnt = 0\n",
        "        if cap.isOpened():\n",
        "            cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            cap.release()\n",
        "        return cnt\n",
        "\n",
        "    def _calculate_num_chunks(self, frame_cnt: int) -> int:\n",
        "        # Stride=2를 고려한 유효 프레임 기반 계산\n",
        "        effective_frames = math.ceil(frame_cnt / 2)\n",
        "        return math.ceil(effective_frames / self.length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        meta = self.samples[idx]\n",
        "        video_path, chunk_idx, label = meta[\"path\"], meta[\"chunk_idx\"], meta[\"label\"]\n",
        "\n",
        "        try:\n",
        "            decoder = VideoDecoder(video_path, device='cpu')\n",
        "            total_frames = decoder.metadata.num_frames or 100000\n",
        "\n",
        "            # Stride 2 적용하여 인덱스 계산\n",
        "            start_frame = chunk_idx * self.length * 2\n",
        "            indices = [min(start_frame + (i * 2), total_frames - 1) for i in range(self.length)]\n",
        "\n",
        "            frames_batch = decoder.get_frames_at(indices=indices)\n",
        "            frames_tensor = frames_batch.data  # (N, C, H, W)\n",
        "\n",
        "            if self.cpu_preprocess:\n",
        "                assert self.preprocessor is not None\n",
        "                processed = self.preprocessor(images=frames_tensor, return_tensors=\"pt\")\n",
        "                frames_tensor = processed.pixel_values  # Shape: (N, C, H, W)\n",
        "\n",
        "            # (N, C, H, W) -> (C, N, H, W)\n",
        "            frames_tensor = frames_tensor.permute(1, 0, 2, 3)\n",
        "\n",
        "            if self.transform:\n",
        "                # Video transform이 필요한 경우 여기서 처리 (Batch 단위 지원 필요)\n",
        "                frames_tensor = self.transform(frames_tensor)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {video_path}: {e}\")\n",
        "            if self.cpu_preprocess:\n",
        "                frames_tensor = torch.zeros((3, self.length, self.size[1], self.size[0]), dtype=torch.float32)\n",
        "            else:\n",
        "                frames_tensor = torch.zeros((3, self.length, self.size[1], self.size[0]), dtype=torch.uint8)\n",
        "\n",
        "\n",
        "\n",
        "        return frames_tensor.contiguous(), torch.tensor(label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "class ImageProcessor:\n",
        "    def __init__(self, naming_fn:Callable[[int], str]):\n",
        "        self.naming_fn = naming_fn\n",
        "\n",
        "    def get_frame_count(self, path: str) -> int:\n",
        "        try:\n",
        "            return len([f for f in os.listdir(path) if f.endswith(('.jpg', '.png'))])\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    def calculate_num_chunks(self, dataset: DeepFakeBaseDataset, frame_cnt: int) -> int:\n",
        "        return math.ceil(frame_cnt / dataset.length)\n",
        "\n",
        "    def getitem(self, dataset: DeepFakeBaseDataset, idx):\n",
        "        meta = dataset.samples[idx]\n",
        "        folder_path, chunk_idx, label, total_frames = meta[\"path\"], meta[\"chunk_idx\"], meta[\"label\"], meta[\"total_frames\"]\n",
        "\n",
        "        frames_list = []\n",
        "        start_frame_idx = chunk_idx * dataset.length\n",
        "\n",
        "        for i in range(dataset.length):\n",
        "            current_idx = min(start_frame_idx + i, total_frames - 1)\n",
        "            file_name = self.naming_fn(current_idx)\n",
        "            img_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "            try:\n",
        "                img_tensor = read_image(img_path, mode=ImageReadMode.RGB)\n",
        "                if dataset.transform:\n",
        "                    img_tensor = dataset.transform(img_tensor)\n",
        "                frames_list.append(img_tensor)\n",
        "            except Exception:\n",
        "                frames_list.append(torch.zeros((3, dataset.size[1], dataset.size[0]), dtype=torch.uint8))\n",
        "\n",
        "        if frames_list:\n",
        "            frames_tensor = torch.stack(frames_list, dim=0) # (N, C, H, W)\n",
        "            if dataset.cpu_preprocess:\n",
        "                assert dataset.preprocessor is not None\n",
        "                processed = dataset.preprocessor(images=frames_tensor, return_tensors=\"pt\")\n",
        "                frames_tensor = processed.pixel_values  # Shape: (N, C, H, W)\n",
        "            frames_tensor = frames_tensor.permute(1, 0, 2, 3) # (C, N, H, W)\n",
        "        else:\n",
        "            if dataset.cpu_preprocess:\n",
        "                frames_tensor = torch.zeros((3, dataset.length, dataset.size[1], dataset.size[0]), dtype=torch.float32)\n",
        "            else:\n",
        "                frames_tensor = torch.zeros((3, dataset.length, dataset.size[1], dataset.size[0]), dtype=torch.uint8)\n",
        "\n",
        "        return frames_tensor.contiguous(), torch.tensor(label)"
      ],
      "metadata": {
        "id": "C5tNYRPu2T8h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0lkjeBIn1ieK"
      },
      "outputs": [],
      "source": [
        "class CelebDFImageDataset(CelebDFBaseDataset):\n",
        "    \"\"\"\n",
        "    이미지 파일이 저장된 폴더에서 프레임을 읽어오는 데이터셋\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            paths: Sequence[Path | str],\n",
        "            length=32, size=(384, 384),\n",
        "            transform=None,\n",
        "            device='cpu',\n",
        "            encoder_model=\"google/siglip2-base-patch16-384\",\n",
        "            cpu_preprocess=False,\n",
        "        ):\n",
        "        super().__init__(\n",
        "            paths,\n",
        "            length,\n",
        "            size,\n",
        "            transform,\n",
        "            device,\n",
        "            encoder_model,\n",
        "            cpu_preprocess\n",
        "        )\n",
        "        self.processor = ImageProcessor(CelebDFImageDataset.idx_to_filename)\n",
        "\n",
        "    @staticmethod\n",
        "    def idx_to_filename(idx: int) -> str:\n",
        "        return f\"frame_{idx+1:06d}.jpg\"\n",
        "\n",
        "    def _get_frame_count(self, path: str) -> int:\n",
        "        return self.processor.get_frame_count(path)\n",
        "\n",
        "    def _calculate_num_chunks(self, frame_cnt: int) -> int:\n",
        "        return self.processor.calculate_num_chunks(self, frame_cnt)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processor.getitem(self, idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "D-Uafut510Lu"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm.notebook import tqdm\n",
        "from shutil import copy2\n",
        "import os\n",
        "\n",
        "def resize_single_video(item):\n",
        "    src_path, dst_path, size = item\n",
        "    # 폴더가 없으면 생성\n",
        "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # FFmpeg 명령어 구성\n",
        "    # -y: 기존 파일 덮어쓰기\n",
        "    # -i: 입력 파일\n",
        "    # -vf: 비디오 필터 (리사이즈)\n",
        "    # -c:v libx264: H.264 코덱 사용\n",
        "    # -crf 23: 일반적인 화질 설정 (낮을수록 고화질)\n",
        "    # -preset veryfast: 속도 우선 인코딩\n",
        "    # -an: 오디오 제거 (학습에 필요 없음, 용량 절감)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-y',\n",
        "        '-hwaccel', 'cuda',             # 디코딩 가속 유지\n",
        "        '-i', str(src_path),\n",
        "        # 성공했던 코드의 필터 형식을 그대로 사용 (flags=lanczos 등)\n",
        "        '-vf', f\"scale={size[0]}:{size[1]}:flags=lanczos\",\n",
        "        '-c:v', 'libx264',              # NVENC 대신 검증된 libx264 사용\n",
        "        '-preset', 'fast',              # 속도 조절\n",
        "        '-crf', '23',                   # 화질 설정\n",
        "        '-pix_fmt', 'yuv420p',          # 재생 호환성을 위해 추가\n",
        "        '-an',                          # 오디오 제거\n",
        "        str(dst_path)\n",
        "    ]\n",
        "\n",
        "    # 실행 (로그는 숨김)\n",
        "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    # ... (cmd 설정 부분)\n",
        "\n",
        "\n",
        "def preprocess_celebdf(src_root, dst_root, size=(384, 384), max_workers=8):\n",
        "    src_root = Path(src_root)\n",
        "    dst_root = Path(dst_root)\n",
        "\n",
        "    if dst_root.exists():\n",
        "        print(f\"Target directory {dst_root} already exists. Skipping preprocessing.\")\n",
        "        return\n",
        "\n",
        "    # 모든 mp4 파일 찾기\n",
        "    video_files = list(src_root.glob(\"*/*.mp4\"))\n",
        "    tasks = []\n",
        "\n",
        "    for src_path in video_files:\n",
        "        rel_path = src_path.relative_to(src_root)\n",
        "        dst_path = dst_root / rel_path\n",
        "        if not dst_path.exists():\n",
        "            tasks.append((src_path, dst_path, size))\n",
        "\n",
        "    print(f\"Starting preprocessing: {len(tasks)} videos...\")\n",
        "\n",
        "    # 멀티프로세싱으로 병렬 처리\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(executor.map(resize_single_video, tasks, chunksize=4), total=len(tasks)))\n",
        "    copy2(src_root / \"List_of_testing_videos.txt\", dst_root / \"List_of_testing_videos.txt\")\n",
        "\n",
        "    print(\"Preprocessing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_a_Hd4cVNV2H"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm.notebook import tqdm\n",
        "from shutil import copy2\n",
        "import os\n",
        "\n",
        "def extract_frames_single_video(item):\n",
        "    src_path, dst_dir, size = item\n",
        "\n",
        "    if dst_dir.exists() and any(dst_dir.iterdir()):\n",
        "        return\n",
        "\n",
        "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    output_pattern = str(dst_dir / \"frame_%06d.jpg\")\n",
        "\n",
        "    # FFmpeg 명령어 변경\n",
        "    # -vf \"select='not(mod(n\\,2))',scale=...\":\n",
        "    #   1. select: 짝수 프레임(0, 2, 4...)만 선택\n",
        "    #   2. scale: 리사이즈\n",
        "    # -vsync vfr: 선택된 프레임만 출력 (타임스탬프 유지를 위해 빈 프레임 생성 방지)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-y',\n",
        "        '-hwaccel', 'cuda',\n",
        "        '-i', str(src_path),\n",
        "        '-vf', rf\"select='not(mod(n\\,2))',scale={size[0]}:{size[1]}:flags=lanczos\",\n",
        "        '-vsync', 'vfr', # Variable Frame Rate: 선택된 프레임만 씀\n",
        "        '-q:v', '2',\n",
        "        output_pattern,\n",
        "        '-threads', '1',\n",
        "        '-hide_banner', '-loglevel', 'error'\n",
        "    ]\n",
        "\n",
        "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "def preprocess_celebdf_frames(src_root, dst_root, size=(384, 384), max_workers=8):\n",
        "    src_root = Path(src_root)\n",
        "    dst_root = Path(dst_root)\n",
        "\n",
        "    video_files = list(src_root.glob(\"*/*.mp4\"))\n",
        "    tasks = []\n",
        "\n",
        "    print(f\"Scanning files...\")\n",
        "    for src_path in video_files:\n",
        "        rel_path = src_path.relative_to(src_root)\n",
        "        dst_dir = dst_root / rel_path.with_suffix('')\n",
        "\n",
        "        if not dst_dir.exists() or not any(dst_dir.iterdir()):\n",
        "            tasks.append((src_path, dst_dir, size))\n",
        "\n",
        "    print(f\"Starting frame extraction (Even frames only): {len(tasks)} videos...\")\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(executor.map(extract_frames_single_video, tasks, chunksize=1), total=len(tasks)))\n",
        "\n",
        "    txt_src = src_root / \"List_of_testing_videos.txt\"\n",
        "    if txt_src.exists():\n",
        "        copy2(txt_src, dst_root / \"List_of_testing_videos.txt\")\n",
        "\n",
        "    print(\"Preprocessing completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAIL_VOS dataset\n",
        "\n",
        "원본 논문은 SAIL\\_VOS\\_3D 데이터셋을 사용하였지만, 쉽사리 얻을 수 있는 데이터셋도 아니고 해서 대신 SAIL\\_VOS로 만족하자."
      ],
      "metadata": {
        "id": "pC2j-lQvUXvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SailVosDataset(DeepFakeBaseDataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            paths: Sequence[Path | str],\n",
        "            length=32, size=(384, 384),\n",
        "            transform=None,\n",
        "            device='cpu',\n",
        "            encoder_model=\"google/siglip2-base-patch16-384\",\n",
        "            cpu_preprocess=False,\n",
        "        ):\n",
        "        self.processor = ImageProcessor(SailVosDataset.idx_to_filename)\n",
        "\n",
        "        super().__init__(\n",
        "            paths,\n",
        "            length,\n",
        "            size,\n",
        "            transform,\n",
        "            device,\n",
        "            encoder_model,\n",
        "            cpu_preprocess\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def idx_to_filename(idx: int) -> str:\n",
        "        return f\"{idx:06d}.png\"\n",
        "\n",
        "    def _get_label(self, path: str) -> int | None:\n",
        "        return 1\n",
        "\n",
        "    def _get_frame_count(self, path: str) -> int:\n",
        "        return self.processor.get_frame_count(path)\n",
        "\n",
        "    def _calculate_num_chunks(self, frame_cnt: int) -> int:\n",
        "        return self.processor.calculate_num_chunks(self, frame_cnt)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processor.getitem(self, idx)"
      ],
      "metadata": {
        "id": "yLtX5deXVwao"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def process_single_image(args):\n",
        "    \"\"\"\n",
        "    하나의 이미지를 처리하는 워커 함수입니다.\n",
        "    multiprocessing에서 호출됩니다.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): (source_path, target_path, target_size)\n",
        "    \"\"\"\n",
        "    src_path, dst_path, target_size = args\n",
        "\n",
        "    try:\n",
        "        # 이미지 읽기\n",
        "        img = cv2.imread(src_path)\n",
        "        if img is None:\n",
        "            return f\"Error reading: {src_path}\"\n",
        "\n",
        "        # 리사이징 (너비, 높이)\n",
        "        # 보간법은 cv2.INTER_LINEAR (기본값) 혹은 축소 시 cv2.INTER_AREA 추천\n",
        "        resized_img = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # 저장할 폴더가 없으면 생성 (Race condition 방지를 위해 exist_ok=True)\n",
        "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
        "\n",
        "        # 이미지 저장\n",
        "        cv2.imwrite(dst_path, resized_img)\n",
        "        return None  # 성공 시 None 반환\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Exception at {src_path}: {str(e)}\"\n",
        "\n",
        "def preprocess_gta_v(source_dir, output_dir, target_size, num_workers=8):\n",
        "    \"\"\"\n",
        "    메인 처리 함수\n",
        "    \"\"\"\n",
        "    source_path = Path(source_dir)\n",
        "    output_path = Path(output_dir)\n",
        "\n",
        "    # 1. 파일 목록 스캔\n",
        "    # 구조: source_dir/Images/video_seq/000000.bmp\n",
        "    # 패턴: 모든 하위 폴더의 images 폴더 안의 bmp 파일들\n",
        "    print(f\"Scanning files in {source_dir}...\")\n",
        "\n",
        "    # glob 패턴: source_dir 아래 모든 폴더(*) 아래 images 폴더 아래 *.bmp\n",
        "    # 혹은 재귀적으로 찾으려면 rglob 사용 가능. 여기서는 데이터셋 구조에 맞춰 명시적으로 찾습니다.\n",
        "    # SAIL-VOS 구조: root -> Images -> vid_seq -> *.bmp\n",
        "    all_files = list(source_path.glob(\"Images/*/*.bmp\"))\n",
        "\n",
        "    if not all_files:\n",
        "        print(\"No files found. Please check the source directory structure.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_files)} images. Preparing tasks...\")\n",
        "\n",
        "    # 2. 작업 리스트 생성\n",
        "    tasks = []\n",
        "\n",
        "    for src_file in all_files:\n",
        "        src_p = Path(src_file)\n",
        "\n",
        "        # 경로 파싱\n",
        "        # src_p: .../ah_1_mcs_1/000000.bmp\n",
        "        # file_name: 000000.bmp\n",
        "        # video_seq_name: ah_1_mcs_1 (부모의 부모 폴더 이름)\n",
        "\n",
        "        file_name = src_p.with_suffix(\".png\").name\n",
        "        video_seq_name = src_p.parent.name\n",
        "\n",
        "        # 목표 경로 생성: output_dir/ah_1_mcs_1/000000.bmp\n",
        "        # (images 폴더 depth를 제거함)\n",
        "        dst_p = output_path / video_seq_name / file_name\n",
        "\n",
        "        tasks.append((str(src_p), str(dst_p), target_size))\n",
        "\n",
        "    # 3. 멀티프로세싱 실행\n",
        "    print(f\"Starting processing with {num_workers} workers...\")\n",
        "    print(f\"Target Size: {target_size}\")\n",
        "\n",
        "    with Pool(processes=num_workers) as pool:\n",
        "        # tqdm을 사용하여 진행률 표시\n",
        "        # imap_unordered가 리스트를 미리 만들지 않아 메모리 효율적이며 순서 상관없이 처리됨\n",
        "        results = list(tqdm(pool.imap_unordered(process_single_image, tasks), total=len(tasks)))\n",
        "\n",
        "    # 4. 결과 리포트\n",
        "    errors = [res for res in results if res is not None]\n",
        "\n",
        "    print(\"\\nProcessing Complete.\")\n",
        "    if errors:\n",
        "        print(f\"{len(errors)} errors occurred:\")\n",
        "        for err in errors[:10]: # 처음 10개만 출력\n",
        "            print(err)\n",
        "        if len(errors) > 10:\n",
        "            print(\"...\")\n",
        "    else:\n",
        "        print(\"Successfully processed all images without errors.\")"
      ],
      "metadata": {
        "id": "JR_318oKaOAu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 모듈"
      ],
      "metadata": {
        "id": "9Zb-IqspVbnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def copy_with_progress(src, dst):\n",
        "    # 파일 크기 확인\n",
        "    file_size = os.path.getsize(src)\n",
        "\n",
        "    # 파일을 바이너리 모드로 열기\n",
        "    with open(src, 'rb') as fsrc:\n",
        "        with open(dst, 'wb') as fdst:\n",
        "            # tqdm 설정 (단위는 바이트, 총 크기 지정)\n",
        "            with tqdm(total=file_size, unit='B', unit_scale=True, desc=os.path.basename(src)) as pbar:\n",
        "                while True:\n",
        "                    # 1MB씩 읽어서 복사\n",
        "                    chunk = fsrc.read(1024 * 1024)\n",
        "                    if not chunk:\n",
        "                        break\n",
        "                    fdst.write(chunk)\n",
        "                    pbar.update(len(chunk)) # 프로그래스바 업데이트\n",
        "\n",
        "\n",
        "def unzip_by_size(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # 전체 파일의 압축 해제 후 총 용량 계산\n",
        "        total_size = sum(file.file_size for file in zip_ref.infolist())\n",
        "\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"해제 중\") as pbar:\n",
        "            for file_info in zip_ref.infolist():\n",
        "                zip_ref.extract(file_info, extract_to)\n",
        "                pbar.update(file_info.file_size) # 각 파일 용량만큼 게이지 증가\n",
        "\n"
      ],
      "metadata": {
        "id": "pGCJlZM8rXz1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ca-lGJcDUxBV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "from typing import TypedDict, Sequence, Literal, Any, NotRequired\n",
        "\n",
        "class CelebDFDataModule(L.LightningDataModule):\n",
        "    def __init__(\n",
        "            self, length=32, batch_size=32, num_workers=8, size=(384, 384),\n",
        "            path=\"/content/preprocessed\", from_img=True, transform=None, val_split_ratio=0.1,\n",
        "            device='cpu', prefetch_factor:int|None=1, pin_memory=True, persistent_workers=True,\n",
        "            encoder_model=\"google/siglip2-base-patch16-384\", cpu_preprocess=False,\n",
        "            use_gta_v=False, gta_v_path=\"/content/gta_v_preprocessed\"\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=['transform'])\n",
        "        self.length = length\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.path = Path(path)\n",
        "        self.dataset_cls: type[DeepFakeBaseDataset] = CelebDFImageDataset\n",
        "        self.dataset_preprocess_method = preprocess_celebdf_frames\n",
        "        self.from_img = from_img\n",
        "        if from_img == False:\n",
        "            self.dataset_cls = CelebDFVideoDataset\n",
        "            self.dataset_preprocess_method = preprocess_celebdf\n",
        "        self.prefetch_factor = prefetch_factor\n",
        "        self.pin_memory = pin_memory\n",
        "        self.val_split_ratio = val_split_ratio\n",
        "        self.size = size\n",
        "        self.persistent_workers = persistent_workers\n",
        "        self.device = device\n",
        "        self.encoder_model = encoder_model\n",
        "        self.cpu_preprocess = cpu_preprocess\n",
        "        self.transform = transform\n",
        "        self.use_gta_v = use_gta_v\n",
        "        self.gta_v_path = Path(gta_v_path)\n",
        "\n",
        "        class LoaderParam(TypedDict):\n",
        "            batch_size: int\n",
        "            num_workers: int\n",
        "            pin_memory: bool\n",
        "            persistent_workers: bool\n",
        "            prefetch_factor: int | None\n",
        "\n",
        "        self._loader_param = LoaderParam(\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=self.pin_memory,\n",
        "            persistent_workers=self.persistent_workers,\n",
        "            prefetch_factor=self.prefetch_factor\n",
        "        )\n",
        "\n",
        "    def prepare_data(self):\n",
        "        path = kagglehub.dataset_download(\"reubensuju/celeb-df-v2\")\n",
        "        self.dataset_preprocess_method(path, self.path, size=self.size)\n",
        "\n",
        "        if self.use_gta_v:\n",
        "            if self.gta_v_path.exists():\n",
        "                print(f\"GTA V data exists in {self.gta_v_path}, skipping preprocessing.\")\n",
        "            else:\n",
        "                # get SAIL-VOS\n",
        "                copy_with_progress(\n",
        "                    \"/content/drive/MyDrive/bootcamp_proj/final/mini-ref-sailvos.zip\",\n",
        "                    \"./mini-ref-sailvos.zip\"\n",
        "                )\n",
        "                unzip_by_size(\"mini-ref-sailvos.zip\", \"./gta-v\")\n",
        "                preprocess_gta_v(Path(\"./gta-v/mini-ref-sailvos\"), self.gta_v_path, self.size)\n",
        "\n",
        "\n",
        "    def _setup_gta_path(self) -> tuple[list[Path], list[Path], list[Path]]:\n",
        "        all_folders_gta =  [\n",
        "            Path(x) for x in self.gta_v_path.glob(\"*\") if os.path.isdir(x)\n",
        "        ]\n",
        "\n",
        "        # 8:1:1 split\n",
        "        train_vid, val_test_vid = train_test_split(\n",
        "            all_folders_gta, test_size = 0.2,\n",
        "            random_state=42,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "        val_vid, test_vid = train_test_split(\n",
        "            val_test_vid, test_size = 0.5,\n",
        "            random_state=42,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "        return train_vid, val_vid, test_vid\n",
        "\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        gta_train_path, gta_val_path, gta_test_path = None, None, None\n",
        "        if self.use_gta_v:\n",
        "            gta_train_path, gta_val_path, gta_test_path = self._setup_gta_path()\n",
        "\n",
        "        class DatasetParam(TypedDict):\n",
        "            length: int\n",
        "            size: int\n",
        "            device: str\n",
        "            encoder_model: str\n",
        "            cpu_preprocess: bool\n",
        "            transform: NotRequired[None | Any]\n",
        "\n",
        "        test_val_param = DatasetParam(\n",
        "            length=self.length,\n",
        "            size=self.size,\n",
        "            device=self.device,\n",
        "            encoder_model=self.encoder_model,\n",
        "            cpu_preprocess=self.cpu_preprocess,\n",
        "        )\n",
        "\n",
        "        train_param: DatasetParam = DatasetParam(\n",
        "            **test_val_param,\n",
        "            transform=self.transform\n",
        "        )\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # 1. 모든 비디오 폴더 경로 가져오기\n",
        "            # 전처리된 이미지 폴더 구조: root/Celeb-real/id0_0000 ...\n",
        "            # glob을 이용해 실제 폴더들을 다 찾습니다.\n",
        "\n",
        "            all_folders = []\n",
        "            if self.from_img:\n",
        "                all_folders = [Path(x) for x in self.path.glob(\"*/*\") if os.path.isdir(x)]\n",
        "            else:\n",
        "                all_folders = [Path(x) for x in self.path.glob(\"*/*.mp4\")]\n",
        "\n",
        "            # 2. Test Set 목록 로드 및 제외\n",
        "            txt_path = self.path / \"List_of_testing_videos.txt\"\n",
        "            test_df = pd.read_csv(txt_path, sep=\" \", header=None, names=[\"label\", \"path\"])\n",
        "            test_paths_set = set()\n",
        "            if self.from_img:\n",
        "                # 확장자를 떼고 비교해야 함 (이미지 폴더명은 확장자가 없으므로)\n",
        "                test_paths_set = set(\n",
        "                    test_df[\"path\"].apply(lambda x: str(self.path / Path(x).with_suffix('')).replace(\"\\\\\", \"/\")).values\n",
        "                )\n",
        "            else:\n",
        "                test_paths_set = set(test_df[\"path\"].apply(lambda x: str(self.path / x)).replace(\"\\\\\", \"/\").values)\n",
        "\n",
        "            train_val_candidates = []\n",
        "            for folder in all_folders:\n",
        "                # 경로 정규화\n",
        "                folder_str = str(folder).replace(\"\\\\\", \"/\")\n",
        "                # 테스트 셋에 포함되지 않은 것만 Train/Val 후보로 등록\n",
        "                if folder_str not in test_paths_set:\n",
        "                     train_val_candidates.append(folder_str)\n",
        "\n",
        "            # 3. 비디오 단위로 Train / Val 분리 (가장 중요!)\n",
        "            # 여기서 쪼개야 비디오 하나가 통째로 Train이나 Val 한쪽으로만 갑니다.\n",
        "            train_videos, val_videos = train_test_split(\n",
        "                train_val_candidates,\n",
        "                test_size=self.val_split_ratio,\n",
        "                random_state=42,\n",
        "                shuffle=True\n",
        "            )\n",
        "\n",
        "            print(f\"Total Train/Val Videos: {len(train_val_candidates)}\")\n",
        "            print(f\"Split result -> Train Videos: {len(train_videos)}, Val Videos: {len(val_videos)}\")\n",
        "\n",
        "            # 4. 데이터셋 인스턴스 생성 (video_paths 주입)\n",
        "            self.celebdf_train = self.dataset_cls(\n",
        "                paths=train_videos,\n",
        "                **train_param,\n",
        "            )\n",
        "\n",
        "            self.celebdf_val = self.dataset_cls(\n",
        "                paths=val_videos,\n",
        "                **test_val_param,\n",
        "            )\n",
        "\n",
        "            if self.use_gta_v:\n",
        "                assert gta_train_path is not None\n",
        "                assert gta_val_path is not None\n",
        "                self.gta_train = SailVosDataset(\n",
        "                    paths=gta_train_path,\n",
        "                    **train_param,\n",
        "                )\n",
        "                self.gta_val = SailVosDataset(\n",
        "                    paths=gta_val_path,\n",
        "                    **test_val_param,\n",
        "                )\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            # 테스트 셋은 기존 로직(txt 파일 기반)을 유지하거나,\n",
        "            # 위에서 test_paths_set을 리스트로 변환해 넘겨줘도 됩니다.\n",
        "            # 여기서는 편의상 기존 클래스가 is_test=True일 때 txt 파일을 읽는 로직을 그대로 쓴다고 가정하거나\n",
        "            # 혹은 위와 똑같이 리스트를 만들어서 넘겨줍니다.\n",
        "\n",
        "            # Test 리스트 생성 로직\n",
        "            txt_path = self.path / \"List_of_testing_videos.txt\"\n",
        "            test_df = pd.read_csv(txt_path, sep=\" \", header=None, names=[\"label\", \"path\"])\n",
        "            test_videos = []\n",
        "            if self.from_img:\n",
        "                test_videos = [\n",
        "                    str(self.path / Path(x).with_suffix('')).replace(\"\\\\\", \"/\")\n",
        "                    for x in test_df[\"path\"].values\n",
        "                ]\n",
        "            else:\n",
        "                test_videos = [str(self.path / Path(x)) for x in test_df[\"path\"].values]\n",
        "            # 실제 존재하는 폴더만 필터링\n",
        "            test_videos = [v for v in test_videos if os.path.exists(v)]\n",
        "\n",
        "            self.celebdf_test = self.dataset_cls(\n",
        "                paths=test_videos,\n",
        "                **test_val_param,\n",
        "            )\n",
        "            if self.use_gta_v:\n",
        "                assert gta_test_path is not None\n",
        "                self.gta_test = SailVosDataset(\n",
        "                    paths=gta_test_path,\n",
        "                    **test_val_param\n",
        "                )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # 2. 현재 훈련 셋(Subset) 내의 클래스별 개수 계산\n",
        "        label_counts = self.celebdf_train.get_label_counter()\n",
        "\n",
        "        target_real_weight = 0.3\n",
        "        target_fake_weight = 0.5\n",
        "        target_gtav_weight = 0.2\n",
        "\n",
        "        class_weights = (target_real_weight / label_counts[0],\n",
        "                         target_fake_weight / label_counts[1])\n",
        "\n",
        "        sample_weights = [class_weights[sample['label']]\n",
        "                          for sample in self.celebdf_train.samples]\n",
        "\n",
        "        dataset = self.celebdf_train\n",
        "\n",
        "        if self.use_gta_v:\n",
        "            dataset = ConcatDataset([dataset, self.gta_train])\n",
        "            label_counts_gta = self.gta_train.get_label_counter()\n",
        "            gtav_weight = target_gtav_weight / label_counts_gta[1]\n",
        "            sample_weights.extend([gtav_weight] * len(self.gta_train))\n",
        "\n",
        "\n",
        "\n",
        "        # 5. 샘플러 생성\n",
        "        # num_samples는 보통 학습 데이터 전체 길이로 설정합니다.\n",
        "        # replacement=True여야 불균형 데이터에서 적은 쪽을 중복해서 뽑아 균형을 맞춥니다.\n",
        "        sampler = WeightedRandomSampler(\n",
        "            weights=sample_weights,\n",
        "            num_samples=min(len(sample_weights), 10000),\n",
        "            replacement=True\n",
        "        )\n",
        "\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            **self._loader_param,\n",
        "            sampler=sampler,\n",
        "        )\n",
        "    def val_dataloader(self):\n",
        "        dataset = self.celebdf_val\n",
        "        if self.use_gta_v:\n",
        "            dataset = ConcatDataset([dataset, self.gta_val])\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            **self._loader_param,\n",
        "        )\n",
        "    def test_dataloader(self):\n",
        "        dataset = self.celebdf_test\n",
        "        if self.use_gta_v:\n",
        "            dataset = ConcatDataset([dataset, self.gta_test])\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            **self._loader_param,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "rxYwEqvnVeQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb8kC4NdXpMX",
        "outputId": "268d5469-6094-4529-c90d-39c5d03686c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhnam0502\u001b[0m (\u001b[33mdhnam0502-likelion\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "wandb_key = userdata.get('wandb_api')\n",
        "wandb.login(key=wandb_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 설정"
      ],
      "metadata": {
        "id": "neA9j4CAVgCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "39efcac3b6c94325a118dc1c5ea13e11",
            "f7bcba4e837442cc97f0233f2090b741",
            "28f92f90bf0b483eb219000abcbb5e73",
            "edb542ff899e4c068a23eb69d6db878e",
            "a1cbf785c8ff498fb98c6388d46671cc",
            "7df54ac369174ead9750c587430a9e62",
            "aba3719e98554c3caf55424518da94c4",
            "c0c39f44870c42f4acd56c848449ad43",
            "a8f288027e344f77a375b88b03756397",
            "3a9a27144c6340a49225f75707d45bfd",
            "64a1479efd0443f58bca459017bb9bce"
          ]
        },
        "id": "TvJmnC34YCFF",
        "outputId": "1ff31123-f8b4-4c13-df80-3ae900a9abd3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/408 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39efcac3b6c94325a118dc1c5ea13e11"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "BATCH_SIZE = 20\n",
        "DECAY_STEPS = (1000 * 32) // BATCH_SIZE\n",
        "LENGTH = 16\n",
        "SIZE = (224, 224)\n",
        "import os\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "transform = v2.Compose([\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "])\n",
        "\n",
        "datamodule = CelebDFDataModule(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=2, # Changed from 8 to 0 to debug worker crash\n",
        "    prefetch_factor=None, # Changed from 2 to None as num_workers=0\n",
        "    device='cpu',\n",
        "    pin_memory=True,\n",
        "    path=\"/content/preprocessed\",\n",
        "    from_img=False,\n",
        "    length=LENGTH,\n",
        "    size=SIZE,\n",
        "    persistent_workers=True,\n",
        "    encoder_model=\"google/siglip2-base-patch16-224\",\n",
        "    cpu_preprocess=True,\n",
        "    transform=transform,\n",
        "    use_gta_v=True,\n",
        "    gta_v_path=\"/content/preprocessed_gta\"\n",
        ")\n",
        "\n",
        "lit_classifier = LitUNITEClassifier(\n",
        "    decay_steps=DECAY_STEPS,\n",
        "    delta_within=(0.01, -2),\n",
        "    delta_between=0.5,\n",
        "    lambda_1=0.5,\n",
        "    lambda_2=0.5,\n",
        "    encoder_model=\"google/siglip2-base-patch16-224\",\n",
        "    use_bfloat=True,\n",
        "    max_len=LENGTH,\n",
        "    cpu_preprocess=True,\n",
        "    dropout=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H637pv3OVro",
        "outputId": "77cc8e81-5c69-4870-e856-2cf1177093f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n",
            "Target directory /content/preprocessed already exists. Skipping preprocessing.\n",
            "GTA V data exists in /content/preprocessed_gta, skipping preprocessing.\n"
          ]
        }
      ],
      "source": [
        "datamodule.prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "pHRx1khcaAoV",
        "outputId": "104a2543-753c-46d3-d2c8-f1da53a74598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>wandb/run-20260205_054036-m6puayr6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/m6puayr6' target=\"_blank\">with_gta_test_baseline_v2</a></strong> to <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/m6puayr6' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/m6puayr6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        }
      ],
      "source": [
        "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from lightning.pytorch.callbacks.lr_monitor import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from lightning.pytorch.callbacks import TQDMProgressBar\n",
        "from lightning.pytorch.profilers import AdvancedProfiler\n",
        "\n",
        "profiler = AdvancedProfiler(\".\", \"profile\")\n",
        "\n",
        "\n",
        "wandb_logger = WandbLogger(\n",
        "    project=\"UNITE_deepfake_classification\",\n",
        "    name=\"with_gta_test_baseline_v2\",\n",
        "    log_model=\"all\",\n",
        "    # id=\"msjnazkr\",\n",
        "    # resume=\"must\",\n",
        ")\n",
        "\n",
        "# ckpt = ModelCheckpoint(monitor=\"val/acc\", mode=\"max\", save_last=True)\n",
        "ckpt_drive = ModelCheckpoint(dirpath=\"/content/drive/MyDrive/bootcamp_proj/final/gtav_test2/\", monitor=\"val/MulticlassAccuracy\", mode=\"max\", save_last=True)\n",
        "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "trainer =  L.Trainer(\n",
        "    max_epochs=20,\n",
        "    # max_steps=100,\n",
        "    # profiler=profiler,\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[ckpt_drive, lr_monitor],\n",
        "    precision='bf16-mixed',\n",
        "    log_every_n_steps=50,\n",
        "    num_sanity_val_steps=0,\n",
        "    # precision=16,\n",
        "    # fast_dev_run=True,\n",
        ")\n",
        "\n",
        "\n",
        "lit_classifier = torch.compile(lit_classifier)\n",
        "wandb_logger.watch(lit_classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "lVG5md6QVkR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707,
          "referenced_widgets": [
            "57d6458608b64fd9bbccf87bd35c99a1",
            "73a532ea3f5a4600ba40f2d3ae8c6ae8"
          ]
        },
        "id": "pNqNPyd7bQsH",
        "outputId": "3b66eb5f-54f5-43c1-9aac-abb9694a2f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n",
            "Target directory /content/preprocessed already exists. Skipping preprocessing.\n",
            "GTA V data exists in /content/preprocessed_gta, skipping preprocessing.\n",
            "Total Train/Val Videos: 6011\n",
            "Split result -> Train Videos: 5409, Val Videos: 602\n",
            "Processing 5409 paths...\n",
            "Loaded 66565 samples from 5409 files/directories.\n",
            "Processing 602 paths...\n",
            "Loaded 7458 samples from 602 files/directories.\n",
            "Processing 54 paths...\n",
            "Loaded 378 samples from 54 files/directories.\n",
            "Processing 7 paths...\n",
            "Loaded 32 samples from 7 files/directories.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:881: Checkpoint directory /content/drive/MyDrive/bootcamp_proj/final/gtav_test2 exists and is not empty.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model        │ UNITE                     │  403 M │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ ce_loss      │ CrossEntropyLoss          │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ ad_loss      │ ADLoss                    │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ val_metrics  │ MetricCollection          │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_roc      │ MulticlassROC             │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics │ MetricCollection          │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ test_roc     │ MulticlassROC             │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ confmat      │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
              "└───┴──────────────┴───────────────────────────┴────────┴───────┴───────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
              "┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model        │ UNITE                     │  403 M │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ ce_loss      │ CrossEntropyLoss          │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ ad_loss      │ ADLoss                    │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ val_metrics  │ MetricCollection          │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_roc      │ MulticlassROC             │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics │ MetricCollection          │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ test_roc     │ MulticlassROC             │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>│ confmat      │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
              "└───┴──────────────┴───────────────────────────┴────────┴───────┴───────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 28.4 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 375 M                                                                                        \n",
              "\u001b[1mTotal params\u001b[0m: 403 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1.6 K                                                                      \n",
              "\u001b[1mModules in train mode\u001b[0m: 74                                                                                          \n",
              "\u001b[1mModules in eval mode\u001b[0m: 312                                                                                          \n",
              "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 28.4 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 375 M                                                                                        \n",
              "<span style=\"font-weight: bold\">Total params</span>: 403 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1.6 K                                                                      \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 74                                                                                          \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 312                                                                                          \n",
              "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57d6458608b64fd9bbccf87bd35c99a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:534: Found 312 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(lit_classifier, datamodule=datamodule)\n",
        "\n",
        "# trainer.fit(lit_classifier, datamodule=datamodule, ckpt_path='/content/drive/MyDrive/bootcamp_proj/final/gtav_test1/last.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "rkwkGvjlbUN6",
        "outputId": "168b935a-64b6-46c1-fd26-6b53c60bcaa6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅██████████</td></tr><tr><td>lr-AdamW</td><td>▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▇▄▅▆▃▅▄▁▄▁▄▄▃▂▂▂▄▂▁▂█▂▂▁▁▃▁▃</td></tr><tr><td>train/loss_ad</td><td>█▇▇▆▅▄▅▄▃▄▃▂▃▂▂▂▃▂▂▂▁▃▁▂▁▁▂▂▂▁</td></tr><tr><td>train/loss_ad/loss_between</td><td>▁▄▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇████▇█████</td></tr><tr><td>train/loss_ad/loss_within</td><td>█▇▇▆▅▄▄▄▃▄▃▂▃▂▂▂▃▂▂▂▁▃▁▂▁▁▂▂▂▁</td></tr><tr><td>train/loss_ce</td><td>▇▃▆▃▄▆▃▅▄▁▄▂▄▄▃▂▂▂▄▂▂▂█▂▂▂▁▃▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>val/MulticlassAUROC</td><td>▁█</td></tr><tr><td>val/MulticlassAccuracy</td><td>▁█</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>lr-AdamW</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.81055</td></tr><tr><td>train/loss_ad</td><td>1.29961</td></tr><tr><td>train/loss_ad/loss_between</td><td>0.25312</td></tr><tr><td>train/loss_ad/loss_within</td><td>1.04649</td></tr><tr><td>train/loss_ce</td><td>0.32148</td></tr><tr><td>trainer/global_step</td><td>1499</td></tr><tr><td>val/MulticlassAUROC</td><td>0.80735</td></tr><tr><td>val/MulticlassAccuracy</td><td>0.71754</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">with_gta_test_baseline_v2</strong> at: <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/jpaq6ohh' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/jpaq6ohh</a><br> View project at: <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification</a><br>Synced 5 W&B file(s), 7 media file(s), 6 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>wandb/run-20260205_050254-jpaq6ohh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th2TtdikB8uj"
      },
      "outputs": [],
      "source": [
        "# del lit_classifier\n",
        "# del trainer\n",
        "# del datamodule\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTDntWy3dyIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91f63985-85bc-4523-a9c8-5cd4f08cf078"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/bootcamp_proj/final/fixed_test3/last.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "trainer.checkpoint_callback.last_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jsHsIls6GJyS",
        "outputId": "79bb3496-404c-4545-8b08-4cef7f347784"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/bootcamp_proj/final/gtav_test1/epoch=4-step=2500.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "trainer.checkpoint_callback.best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "71d2ec984d244fa2beb16488159ac51c",
            "cc0e0b530d524b4fb7f1a4009b3cdd09",
            "8476ee1628d44f8ca8fe470f4929f83f",
            "0610c24e21bc42ecb84875b4906f8f28",
            "d2f1c5a8e77e4cdaba047cc32c185acd",
            "4ce37317bcba43ffa5fa1a1728074310",
            "5d5cdb5c887b495486db30128db7672e",
            "abfa6f4d809043938a8ea45d12ea559a",
            "c0c2f55c5e6b44ddb724e56d635ef05c",
            "153ab894cf344433a6736bcdb9fbda49",
            "58a4e6e886204aca9d0d3308dfeca1f7"
          ]
        },
        "id": "X9nnHVLJAhwW",
        "outputId": "fe2c2374-f933-40c0-f75c-513f3313536b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/408 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71d2ec984d244fa2beb16488159ac51c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "lit_classifier = LitUNITEClassifier.load_from_checkpoint('/content/drive/MyDrive/bootcamp_proj/final/fixed_test3/epoch=0-step=3329.ckpt', strict=False, weights_only=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트"
      ],
      "metadata": {
        "id": "D4eE6OqJVq_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "Yn1giSd-2JLs",
        "outputId": "c3d51e5a-2f83-484c-eb89-a1c2cc59e710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n",
            "Target directory /content/preprocessed already exists. Skipping preprocessing.\n",
            "GTA V data exists in /content/preprocessed_gta, skipping preprocessing.\n",
            "Processing 518 paths...\n",
            "Loaded 6442 samples from 518 files/directories.\n",
            "Processing 7 paths...\n",
            "Loaded 35 samples from 7 files/directories.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UsageError",
          "evalue": "Run (msjnazkr) is finished. The call to `_config_callback` will be ignored. Please make sure that you are using an active run.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUsageError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2286340077.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlit_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule, weights_only)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule, weights_only)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         )\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;31m# remove the tensors from the test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_tensors_to_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path, weights_only)\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;31m# only log hparams if enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_autolog_hparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m             \u001b[0m_log_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_setup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loggers/utilities.py\u001b[0m in \u001b[0;36m_log_hyperparams\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams_initial\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning_utilities/core/rank_zero.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `rank_zero_only.rank` needs to be set before use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loggers/wandb.py\u001b[0m in \u001b[0;36mlog_hyperparams\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_callable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_json_serializable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_val_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_config.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, d, allow_val_change)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0msanitized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_val_change\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msanitized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwb_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUsageError\u001b[0m: Run (msjnazkr) is finished. The call to `_config_callback` will be ignored. Please make sure that you are using an active run."
          ]
        }
      ],
      "source": [
        "trainer.test(model=lit_classifier, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZcUlpmlcfSy",
        "outputId": "710936f2-016e-4ccb-9b1b-a636db88e533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "!ffmpeg -hwaccels | grep cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75LctAT8HiSk"
      },
      "outputs": [],
      "source": [
        "!cp /content/UNITE_deepfake_classification/93i7lafx/checkpoints/last.ckpt /content/drive/MyDrive/bootcamp_proj/final/v2/last.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbY_kyav3WY6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39efcac3b6c94325a118dc1c5ea13e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7bcba4e837442cc97f0233f2090b741",
              "IPY_MODEL_28f92f90bf0b483eb219000abcbb5e73",
              "IPY_MODEL_edb542ff899e4c068a23eb69d6db878e"
            ],
            "layout": "IPY_MODEL_a1cbf785c8ff498fb98c6388d46671cc"
          }
        },
        "f7bcba4e837442cc97f0233f2090b741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df54ac369174ead9750c587430a9e62",
            "placeholder": "​",
            "style": "IPY_MODEL_aba3719e98554c3caf55424518da94c4",
            "value": "Loading weights: 100%"
          }
        },
        "28f92f90bf0b483eb219000abcbb5e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c39f44870c42f4acd56c848449ad43",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8f288027e344f77a375b88b03756397",
            "value": 408
          }
        },
        "edb542ff899e4c068a23eb69d6db878e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9a27144c6340a49225f75707d45bfd",
            "placeholder": "​",
            "style": "IPY_MODEL_64a1479efd0443f58bca459017bb9bce",
            "value": " 408/408 [00:00&lt;00:00, 801.28it/s, Materializing param=vision_model.post_layernorm.weight]"
          }
        },
        "a1cbf785c8ff498fb98c6388d46671cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df54ac369174ead9750c587430a9e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba3719e98554c3caf55424518da94c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0c39f44870c42f4acd56c848449ad43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f288027e344f77a375b88b03756397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a9a27144c6340a49225f75707d45bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64a1479efd0443f58bca459017bb9bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57d6458608b64fd9bbccf87bd35c99a1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_73a532ea3f5a4600ba40f2d3ae8c6ae8",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 0/19 \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 0/500 \u001b[2m0:00:00 • -:--:--\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3m \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 0/19 <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 0/500 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 • -:--:--</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span> <span style=\"font-style: italic\"> </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "73a532ea3f5a4600ba40f2d3ae8c6ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71d2ec984d244fa2beb16488159ac51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc0e0b530d524b4fb7f1a4009b3cdd09",
              "IPY_MODEL_8476ee1628d44f8ca8fe470f4929f83f",
              "IPY_MODEL_0610c24e21bc42ecb84875b4906f8f28"
            ],
            "layout": "IPY_MODEL_d2f1c5a8e77e4cdaba047cc32c185acd"
          }
        },
        "cc0e0b530d524b4fb7f1a4009b3cdd09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ce37317bcba43ffa5fa1a1728074310",
            "placeholder": "​",
            "style": "IPY_MODEL_5d5cdb5c887b495486db30128db7672e",
            "value": "Loading weights: 100%"
          }
        },
        "8476ee1628d44f8ca8fe470f4929f83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abfa6f4d809043938a8ea45d12ea559a",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0c2f55c5e6b44ddb724e56d635ef05c",
            "value": 408
          }
        },
        "0610c24e21bc42ecb84875b4906f8f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_153ab894cf344433a6736bcdb9fbda49",
            "placeholder": "​",
            "style": "IPY_MODEL_58a4e6e886204aca9d0d3308dfeca1f7",
            "value": " 408/408 [00:00&lt;00:00, 818.10it/s, Materializing param=vision_model.post_layernorm.weight]"
          }
        },
        "d2f1c5a8e77e4cdaba047cc32c185acd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ce37317bcba43ffa5fa1a1728074310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5cdb5c887b495486db30128db7672e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abfa6f4d809043938a8ea45d12ea559a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c2f55c5e6b44ddb724e56d635ef05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "153ab894cf344433a6736bcdb9fbda49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a4e6e886204aca9d0d3308dfeca1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
