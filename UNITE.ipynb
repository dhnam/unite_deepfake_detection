{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋은 우선적으로는 CelebDFV2를 사용하고, 이후에 GTA-V 데이터를 합쳐보도록 하자."
      ],
      "metadata": {
        "id": "LbmeSoy6C9Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이걸 어디부터 구현해야 할까...\n",
        "\n",
        "일단 SigLIP을 불러오는 것부터 시작하자. Quantize를 해줘서 최대한 부담을 줄여주자."
      ],
      "metadata": {
        "id": "YpBO_bgZCCur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --system lightning\n",
        "!pip install -q lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvXKooN5CCOG",
        "outputId": "804e1302-ae91-4c7c-8bc1-55e944dd2d48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m43 packages\u001b[0m \u001b[2min 327ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 87ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning-utilities\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytorch-lightning\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchmetrics\u001b[0m\u001b[2m==1.8.2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --system bitsandbytes\n",
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5NX_61lI4WV",
        "outputId": "38edbe63-383f-4f8b-dd62-6b5742343c1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m29 packages\u001b[0m \u001b[2min 32ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 731ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.49.1\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --system decord\n",
        "!pip install -q decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXtdBnVtErSJ",
        "outputId": "3195834a-f973-41ed-f62b-c4d95dc7928e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 146ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdecord\u001b[0m\u001b[2m==0.6.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CesM4HbVBZWY"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as L\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "embed_size = 768\n",
        "\n",
        "frame_token = 576"
      ],
      "metadata": {
        "id": "G1LPodHaYK4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHpA0CA-MaC_",
        "outputId": "86ad4985-09c3-4c93-e152-8ae990927634"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "솔직히, 왠만하면 원본 모델을 따라가고 싶지만... 일단 인코더를 더 경량으로 바꾼다.\n",
        "\n",
        "나중에 성능 안 나오면 탓할 것 중 인코더가 늘었다."
      ],
      "metadata": {
        "id": "RZwnOwZsXtZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "cTmv1qBoXnHs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "        assert self.head_dim * num_heads == embed_size, \"embed_size must be divisible by num_heads\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.k_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.v_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.dropout_p = dropout\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x, return_head_contrib=False):\n",
        "        # x: (batch_size, seq_len, embed_size)\n",
        "        batch_size, seq_len, embed_size = x.shape\n",
        "\n",
        "        # Project queries, keys, values\n",
        "        q = self.q_proj(x) # (batch_size, seq_len, embed_size)\n",
        "        k = self.k_proj(x) # (batch_size, seq_len, embed_size)\n",
        "        v = self.v_proj(x) # (batch_size, seq_len, embed_size)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, seq_len, head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, seq_len, head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Apply scaled dot product attention\n",
        "        # dropout_p는 훈련 중일 때만 적용\n",
        "        attn_output_raw = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=self.dropout_p if self.training else 0.0,\n",
        "            is_causal=False\n",
        "        )\n",
        "        # attn_output: (b, num_heads, seq_len, head_dim)\n",
        "\n",
        "\n",
        "        head_contrib = None\n",
        "        if return_head_contrib:\n",
        "            # out_proj_weight: [embed_size, embed_size] -> [num_heads, head_dim, embed_size]\n",
        "            out_proj_weight = self.out_proj.weight.t().view(self.num_heads, self.head_dim, embed_size)\n",
        "            # b: batch size, h: num_head, l: seq_len, k: head_dim, d: embed_dim\n",
        "            head_contrib = torch.einsum(\"bhlk, hkd -> bhld\", attn_output_raw, out_proj_weight)\n",
        "            # -> result: [batch, num_head, seq_len, embed_dim] (-> [batch, num_head, seq_len, embed_dim])\n",
        "            # Have to remove class token, and pool that with frame dimension\n",
        "\n",
        "        # Concatenate heads and apply final linear projection\n",
        "        attn_output = attn_output_raw.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_size)\n",
        "        attn_output = self.out_proj(attn_output) # (batch_size, seq_len, embed_size)\n",
        "\n",
        "\n",
        "        x = self.ln1(x + attn_output) # Residual connection + LayerNorm\n",
        "        x = self.ln2(x + self.mlp(x))\n",
        "\n",
        "        if return_head_contrib:\n",
        "            return x, head_contrib\n",
        "        return x"
      ],
      "metadata": {
        "id": "0p36S4LXYIzn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float=0.1, max_len: int=32): # max_len을 32로 설정\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        # d_model에 맞춰 div_term 계산\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # [max_len, d_model] 형상으로 생성\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # [1, max_len, d_model]로 변경하여 Batch First 대응\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [batch_size, frame * seq_len, d_model]\n",
        "        \"\"\"\n",
        "        b, f, t, d = x.shape\n",
        "        # [1, max_len, 1, d_model]로 -> frame과 d_model에만 적용한다\n",
        "        # seq_len의 공간적 위치 정보는 이미 인코더에서 처리해주었다.\n",
        "        curr_pe = self.pe[:, :f, :].unsqueeze(2)\n",
        "        # 입력된 x의 길이만큼만 PE를 더함\n",
        "        x = x + curr_pe\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "IA7_g3dQb3rI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "class GPUSigLIPProcessor:\n",
        "    def __init__(self, processor):\n",
        "        config = processor.image_processor\n",
        "\n",
        "        # 1. 리사이즈 설정: Bilinear + Antialias=True가 핵심\n",
        "        # Fast 프로세서가 텐서를 처리할 때 사용하는 로직과 일치시킵니다.\n",
        "        self.resize = v2.Resize(\n",
        "            size=(config.size['height'], config.size['width']),\n",
        "            interpolation=v2.InterpolationMode.BILINEAR, # resample=2\n",
        "            antialias=True # 오차를 줄이는 가장 중요한 설정\n",
        "        )\n",
        "\n",
        "        # 2. 정규화 설정\n",
        "        # (x - 0.5) / 0.5 연산\n",
        "        self.mean = torch.tensor(config.image_mean).view(1, 3, 1, 1)\n",
        "        self.std = torch.tensor(config.image_std).view(1, 3, 1, 1)\n",
        "        self.rescale_factor = config.rescale_factor\n",
        "\n",
        "    def __call__(self, video_tensor):\n",
        "        \"\"\"\n",
        "        video_tensor: (B, 3, T, H, W), uint8, GPU\n",
        "        \"\"\"\n",
        "        b, c, t, h, w = video_tensor.shape\n",
        "        device = video_tensor.device\n",
        "\n",
        "        # 차원 변경 (B*T, C, H, W)\n",
        "        x = video_tensor.permute(0, 2, 1, 3, 4)\n",
        "        x = x.flatten(0, 1)\n",
        "\n",
        "        # [Step 1] Resize (uint8 상태에서 수행하거나 float32에서 수행)\n",
        "        # torchvision v2는 uint8 입력을 받아 내부적으로 고정밀 연산을 수행합니다.\n",
        "        x = self.resize(x)\n",
        "\n",
        "        # [Step 2] Float32 변환 및 Rescale (0~255 -> 0~1)\n",
        "        x = x.to(torch.float32) * self.rescale_factor\n",
        "\n",
        "        # [Step 3] Normalize (x - 0.5) / 0.5\n",
        "        # mean, std를 캐싱하여 속도 최적화\n",
        "        self.mean = self.mean.to(device)\n",
        "        self.std = self.std.to(device)\n",
        "        x = (x - self.mean) / self.std\n",
        "\n",
        "        # [Step 4] 최종 모델 입력형태인 float16으로 반환\n",
        "        return x.to(torch.float16)"
      ],
      "metadata": {
        "id": "Fx_U3LxUoMqM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNITE(nn.Module):\n",
        "    def __init__(self, num_channel=3, num_cls=2, num_heads=12, max_len=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        model_id = \"google/siglip2-base-patch16-384\"\n",
        "        self.vis_encoder = AutoModel.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            dtype=torch.bfloat16,\n",
        "            attn_implementation=\"sdpa\",\n",
        "        )\n",
        "        self.embed_size = self.vis_encoder.config.vision_config.hidden_size\n",
        "        processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
        "        self.processor = GPUSigLIPProcessor(processor)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        for para in self.vis_encoder.parameters():\n",
        "            para.requires_grad = False\n",
        "        self.vis_encoder.eval()\n",
        "\n",
        "        self.class_token = nn.Parameter(torch.randn((self.embed_size,)), requires_grad=True)\n",
        "\n",
        "        self.pos_embedding = PositionalEncoding(self.embed_size)\n",
        "        self.first_encoder = ViTEncoder(self.embed_size, num_heads, dropout)\n",
        "        self.encoders = nn.ModuleList([ViTEncoder(self.embed_size, num_heads, dropout) for _ in range(3)])\n",
        "        self.mlp_head = nn.Linear(self.embed_size, num_cls)\n",
        "\n",
        "\n",
        "    def forward(self, x, return_ad_param=False):\n",
        "        self.vis_encoder.eval()\n",
        "\n",
        "        # Input: [batch, c, frame, h, w]\n",
        "        b, _, f, *_ = x.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # -> Preprocessing [batch * frame, c, h, w]\n",
        "            x = self.processor(x)\n",
        "            # -> Visual encoding [batch * frame, token/frame(576), dim/token (embed_size)]\n",
        "            x = self.vis_encoder.vision_model(pixel_values=x).last_hidden_state\n",
        "        # -> [batch, frame, token/frame, dim/token]\n",
        "        x = x.reshape(b, f, -1, self.embed_size)\n",
        "        x = self.pos_embedding(x)\n",
        "        train_in = x # xi\n",
        "\n",
        "        _, _, t, d = x.shape\n",
        "        # Reshape for transformer\n",
        "        # -> [batch, total token, dim/token]\n",
        "        x = x.reshape(b, f*t, d)\n",
        "        # Add class token\n",
        "        cls_token = self.class_token.view(1, 1, -1).expand(b, -1, -1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        P = None\n",
        "        if return_ad_param:\n",
        "            x, head_contrib = self.first_encoder(x, return_head_contrib=True)\n",
        "            # head_contrib: [batch, head, total token + class token, dim/token]\n",
        "            # -> [batch, head, total token, dim/token]\n",
        "            head_contrib = head_contrib[:, :, 1:, :]\n",
        "            # -> [batch, head, frame, token/frame, dim/token]\n",
        "            head_contrib = head_contrib.view(b, self.num_heads, f, t, d)\n",
        "            # -> mean pooling [batch, head, token/frame, dim/token]\n",
        "            head_contrib = head_contrib.mean(dim=2) # A\n",
        "\n",
        "            P = torch.einsum(\"bftd, bhtd -> bhf\", train_in, head_contrib)\n",
        "        else:\n",
        "            x = self.first_encoder(x)\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "\n",
        "        # Get only cls_token\n",
        "        x = x[:, 0, :]\n",
        "        x = x.view(b, -1)\n",
        "        x = self.mlp_head(x)\n",
        "        if return_ad_param:\n",
        "            return x, P\n",
        "        return x"
      ],
      "metadata": {
        "id": "Dl5XX6bXbf46"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ADLoss(nn.Module):\n",
        "    def __init__(self, num_cls=2, num_heads=12, max_len=32, delta_within=(0.01, -2.0), delta_between=0.5, eta=0.05):\n",
        "        super().__init__()\n",
        "        # C shape: [num_classes, num_heads, max_len]\n",
        "        # 논문 식(3)에 따라 센터를 각 클래스별로 유지해야 함\n",
        "        C = torch.zeros(num_cls, num_heads, max_len)\n",
        "        self.register_buffer('C', C)\n",
        "\n",
        "        self.num_cls = num_cls\n",
        "        self.delta_within = torch.tensor(delta_within) # [0.01, -2.0] (True, Fake)\n",
        "        self.delta_between = delta_between # 0.5\n",
        "        self.eta = eta\n",
        "\n",
        "    def forward(self, P, labels, log_detail=False):\n",
        "        \"\"\"\n",
        "        P: [batch, num_heads, max_len] (Pooled Features)\n",
        "        labels: [batch] (Class indices)\n",
        "        \"\"\"\n",
        "        device = P.device\n",
        "        self.delta_within = self.delta_within.to(device)\n",
        "\n",
        "        P_norm = F.normalize(P.view(P.size(0), -1), p=2, dim=1).view_as(P)\n",
        "        C_norm = F.normalize(self.C.view(self.num_cls, -1), p=2, dim=1).view_as(self.C)\n",
        "\n",
        "        # --- 1. 센터 업데이트 (식 3) ---\n",
        "        # 배치의 각 클래스별 평균을 구해서 업데이트\n",
        "        for c in range(self.num_cls):\n",
        "            mask = (labels == c)\n",
        "            if mask.any():\n",
        "                # 해당 클래스의 이번 배치 평균\n",
        "                batch_class_mean = P_norm[mask].mean(dim=0) # [num_heads, max_len]\n",
        "                # print(f\"{self.C[c].shape, P.shape, P[mask].shape, batch_class_mean.shape=}\")\n",
        "                # 이동 평균 업데이트\n",
        "                with torch.no_grad():\n",
        "                    self.C[c] = (1 - self.eta) * self.C[c] + self.eta * batch_class_mean.detach()\n",
        "\n",
        "        # --- 2. Within-class Loss (식 4) ---\n",
        "        # 각 샘플과 자기 클래스 센터 사이의 거리\n",
        "        # P: [B, H, F], self.C[labels]: [B, H, F]\n",
        "        diff_within = P_norm - C_norm[labels]\n",
        "        # L2 Norm 계산 (헤드와 프레임 차원에 대해)\n",
        "        dist_within = torch.norm(diff_within, p=2, dim=(1, 2))\n",
        "\n",
        "        # 각 샘플별 delta 적용\n",
        "        loss_within = torch.relu(dist_within - self.delta_within[labels]).mean()\n",
        "\n",
        "        # --- 3. Between-class Loss (식 5: 서로 다른 쌍에 대해 전부) ---\n",
        "        # 클래스 센터들 간의 모든 쌍 거리 (Pairwise Distance) 계산\n",
        "        # self.C: [num_classes, H*F]로 펼쳐서 계산하면 편리함\n",
        "        C_flat = C_norm.view(self.num_cls, -1)\n",
        "\n",
        "        # 모든 클래스 쌍 간의 차이 계산: [num_classes, num_classes, H*F]\n",
        "        # Broadcasting 활용: (N, 1, D) - (1, N, D) -> (N, N, D)\n",
        "        diff_between = C_flat.unsqueeze(1) - C_flat.unsqueeze(0)\n",
        "\n",
        "        # 모든 쌍의 거리 행렬: [num_classes, num_classes]\n",
        "        dist_matrix = torch.norm(diff_between, p=2, dim=2)\n",
        "\n",
        "        # k != l 인 조건 (서로 다른 쌍) 추출을 위한 마스크\n",
        "        # torch.triu를 써서 중복 계산(k,l과 l,k)을 피하고 자기 자신(k=l)도 제외함\n",
        "        mask_between = torch.triu(torch.ones(self.num_cls, self.num_cls, device=device), diagonal=1).bool()\n",
        "\n",
        "        # 서로 다른 클래스 쌍의 거리들만 추출\n",
        "        different_pairs_dist = dist_matrix[mask_between]\n",
        "\n",
        "        # 식 (5) 적용: max(delta - dist, 0)\n",
        "        loss_between = torch.relu(self.delta_between - different_pairs_dist).sum()\n",
        "        if log_detail:\n",
        "            return loss_within + loss_between, loss_within, loss_between\n",
        "\n",
        "        return loss_within + loss_between"
      ],
      "metadata": {
        "id": "OflIU8asnbGT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import Accuracy, AveragePrecision, Precision, Recall"
      ],
      "metadata": {
        "id": "w_3YGmU93VT8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitUNITEClassifier(L.LightningModule):\n",
        "    def __init__(\n",
        "            self, num_cls=2, num_heads=12, max_len=32, dropout=0.1,\n",
        "            delta_within=(0.01, -2.0), delta_between=0.5, eta=0.05,\n",
        "            lambda_1=0.5, lambda_2=0.5, lr=1e-4, decay_steps=1000,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = UNITE(\n",
        "            num_cls=num_cls,\n",
        "            num_heads=num_heads,\n",
        "            max_len=max_len,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.ad_loss = ADLoss(\n",
        "            num_cls=num_cls,\n",
        "            num_heads=num_heads,\n",
        "            max_len=max_len,\n",
        "            delta_within=delta_within,\n",
        "            delta_between=delta_between,\n",
        "            eta=eta,\n",
        "        )\n",
        "        self.lambda_1 = lambda_1\n",
        "        self.lambda_2 = lambda_2\n",
        "\n",
        "        self.lr = lr\n",
        "        self.decay_steps = decay_steps # Set this in respect to batch size; original batch size was 32\n",
        "\n",
        "        self.acc = Accuracy(task='multiclass', num_classes=num_cls)\n",
        "        self.ap = AveragePrecision(task='multiclass', num_classes=num_cls)\n",
        "        self.precision = Precision(task='multiclass', num_classes=num_cls)\n",
        "        self.recall = Recall(task='multiclass', num_classes=num_cls)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit, P = self.model(x, return_ad_param=True)\n",
        "        loss_ad, within, between = self.ad_loss(P, y, log_detail=True)\n",
        "        loss_ce = self.ce_loss(logit, y)\n",
        "        loss = loss_ce * self.lambda_1 + loss_ad * self.lambda_2\n",
        "        self.log(\"train/loss_ad\", loss_ad, logger=True)\n",
        "        self.log(\"train/loss_ad/loss_within\", within, logger=True)\n",
        "        self.log(\"train/loss_ad/loss_between\", between, logger=True)\n",
        "        self.log(\"train/loss_ce\", loss_ce, logger=True)\n",
        "        self.log(\"train/loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit, P = self.model(x, return_ad_param=True)\n",
        "        loss_ad = self.ad_loss(P, y)\n",
        "        loss_ce = self.ce_loss(logit, y)\n",
        "        loss = loss_ce * self.lambda_1 + loss_ad * self.lambda_2\n",
        "        self.log(\"val/loss_ad\", loss_ad, logger=True)\n",
        "        self.log(\"val/loss_ce\", loss_ce, logger=True)\n",
        "        self.log(\"val/loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        self.acc(logit, y)\n",
        "        self.ap(logit, y)\n",
        "        self.precision(logit, y)\n",
        "        self.recall(logit, y)\n",
        "        self.log(\"val/acc\", self.acc, logger=True)\n",
        "        self.log(\"val/ap\", self.ap, logger=True)\n",
        "        self.log(\"val/precision\", self.precision, logger=True)\n",
        "        self.log(\"val/recall\", self.recall, logger=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit = self.model(x)\n",
        "\n",
        "        self.acc(logit, y)\n",
        "        self.ap(logit, y)\n",
        "        self.precision(logit, y)\n",
        "        self.recall(logit, y)\n",
        "        self.log(\"test/acc\", self.acc, logger=True)\n",
        "        self.log(\"test/ap\", self.ap, logger=True)\n",
        "        self.log(\"test/precision\", self.precision, logger=True)\n",
        "        self.log(\"test/recall\", self.recall, logger=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optim, self.decay_steps, gamma=0.5)\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optim,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\",\n",
        "            },\n",
        "        }"
      ],
      "metadata": {
        "id": "Mwpa-9_OuUpJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from decord import VideoReader, cpu\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "\n",
        "class CelebDFDataset(Dataset):\n",
        "    def __init__(self, is_test: bool, path_str:str, length=32, size=(384, 384), transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            is_test (bool): True면 테스트 셋, False면 트레인 셋 로드\n",
        "            length (int): 시퀀스 길이 (기본 32)\n",
        "            transform: 이미지 전처리 (Optional)\n",
        "        \"\"\"\n",
        "\n",
        "        self.path = Path(path_str)\n",
        "        self.is_test = is_test\n",
        "        self.length = length\n",
        "        self.size = size\n",
        "        self.transform = transform\n",
        "\n",
        "        # 모든 mp4 파일 검색\n",
        "        self.files = list(self.path.glob(\"*/*.mp4\"))\n",
        "\n",
        "        # 테스트 비디오 리스트 로드\n",
        "        # 파일 형식: [1|0] [path] (예: 1 YouTube-real/00170.mp4)\n",
        "        txt_path = self.path / \"List_of_testing_videos.txt\"\n",
        "        test_df = pd.read_csv(txt_path, sep=\" \", header=None, names=[\"label\", \"path\"])\n",
        "\n",
        "        # 비교를 위해 테스트 파일 경로들을 Set으로 변환 (검색 속도 향상)\n",
        "        # Windows/Linux 경로 구분자 통일을 위해 '/'로 replace 처리\n",
        "        test_files_set = set(test_df[\"path\"].apply(lambda x: x.replace(\"\\\\\", \"/\")).values)\n",
        "\n",
        "        self.samples = [] # (video_path, chunk_index, label) 튜플을 저장할 리스트\n",
        "\n",
        "        print(f\"Processing metadata for {'Test' if is_test else 'Train'} set...\")\n",
        "\n",
        "        for next_file in self.files:\n",
        "            # 데이터셋 루트 기준 상대 경로 (예: YouTube-real/00170.mp4)\n",
        "            rel_path = str(next_file.relative_to(self.path)).replace(\"\\\\\", \"/\")\n",
        "\n",
        "            # 현재 파일이 테스트 리스트에 있는지 확인\n",
        "            is_in_test_list = rel_path in test_files_set\n",
        "\n",
        "            # 요청한 Split(Train/Test)과 맞지 않으면 스킵\n",
        "            if self.is_test != is_in_test_list:\n",
        "                continue\n",
        "\n",
        "            # 레이블 결정 (0: Real, 1: Fake)\n",
        "            # 폴더명 기반 판단\n",
        "            if \"YouTube-real\" in rel_path or \"Celeb-real\" in rel_path:\n",
        "                label = 0\n",
        "            elif \"Celeb-synthesis\" in rel_path:\n",
        "                label = 1\n",
        "            else:\n",
        "                continue # 알 수 없는 폴더는 제외\n",
        "\n",
        "            cap = cv2.VideoCapture(str(next_file))\n",
        "            if cap.isOpened():\n",
        "                frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "                cap.release()\n",
        "\n",
        "                if frame_cnt <= 0: continue\n",
        "\n",
        "                # 영상 하나를 여러 데이터(Chunk)로 쪼개기\n",
        "                # Stride=2 (하나 걸러 하나), Length=32\n",
        "                # 데이터 하나당 필요한 원본 프레임 구간 = 약 64 프레임\n",
        "                # 전체 프레임에서 Stride 2로 뽑았을 때 나오는 유효 프레임 수\n",
        "                effective_frames = math.ceil(frame_cnt / 2)\n",
        "\n",
        "                # 영상 하나에서 나오는 데이터 개수 (올림 처리)\n",
        "                num_chunks = math.ceil(effective_frames / self.length)\n",
        "\n",
        "                for i in range(num_chunks):\n",
        "                    self.samples.append({\n",
        "                        \"video_path\": str(next_file),\n",
        "                        \"chunk_idx\": i,\n",
        "                        \"label\": label\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"Cannot open video: {next_file}\")\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} samples from {len(self.files)} files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        meta = self.samples[idx]\n",
        "        video_path = meta[\"video_path\"]\n",
        "        label = meta[\"label\"]\n",
        "        chunk_idx = meta[\"chunk_idx\"]\n",
        "\n",
        "        try:\n",
        "            # 1. VideoReader 객체 생성 (CPU 컨텍스트 사용)\n",
        "            # num_threads=0은 가용한 모든 코어를 사용하여 디코딩함을 의미합니다.\n",
        "            # vr = VideoReader(video_path, ctx=cpu(0), num_threads=0, width=self.size[0], height=self.size[1])\n",
        "            vr = VideoReader(video_path, ctx=cpu(0), num_threads=0)\n",
        "            total_frames = len(vr)\n",
        "\n",
        "            # 2. 가져올 프레임 인덱스 계산 (Stride 2 적용)\n",
        "            start_frame = chunk_idx * self.length * 2\n",
        "            indices = []\n",
        "            for i in range(self.length):\n",
        "                idx_to_fetch = start_frame + (i * 2)\n",
        "                # 영상 길이를 넘어갈 경우 마지막 프레임으로 패딩\n",
        "                if idx_to_fetch < total_frames:\n",
        "                    indices.append(idx_to_fetch)\n",
        "                else:\n",
        "                    indices.append(total_frames - 1)\n",
        "\n",
        "            # 3. 한 번에 여러 프레임 가져오기 (가장 핵심적인 속도 향상 포인트)\n",
        "            # get_batch 결과는 [T, H, W, C] 형태의 decord.NDArray입니다.\n",
        "            frames = vr.get_batch(indices).asnumpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {video_path}: {e}\")\n",
        "            # 에러 발생 시 0으로 채워진 텐서 반환 (학습 중단 방지)\n",
        "            frames = np.zeros((self.length, self.size[1], self.size[0], 3), dtype=np.uint8)\n",
        "\n",
        "        # 4. Resize 처리 (Batch 단위 처리를 위해 루프 사용)\n",
        "        # 만약 torchvision v2를 사용한다면 텐서 상태에서 배치로 resize 하는 것이 더 빠릅니다.\n",
        "        # resized_frames = []\n",
        "        # for frame in frames:\n",
        "            # resized_frames.append(cv2.resize(frame, self.size))\n",
        "\n",
        "        # frames_np = np.array(resized_frames)\n",
        "        frames_np = np.array(frames)\n",
        "\n",
        "        # 5. Tensor 변환 및 차원 변경: (T, H, W, C) -> (C, T, H, W)\n",
        "        frames_tensor = torch.from_numpy(frames_np).permute(3, 0, 1, 2)\n",
        "\n",
        "        return frames_tensor.contiguous(), torch.tensor(label)"
      ],
      "metadata": {
        "id": "mCig1aG-MBvI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from shutil import copy2\n",
        "import os\n",
        "\n",
        "def resize_single_video(item):\n",
        "    src_path, dst_path, size = item\n",
        "    # 폴더가 없으면 생성\n",
        "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # FFmpeg 명령어 구성\n",
        "    # -y: 기존 파일 덮어쓰기\n",
        "    # -i: 입력 파일\n",
        "    # -vf: 비디오 필터 (리사이즈)\n",
        "    # -c:v libx264: H.264 코덱 사용\n",
        "    # -crf 23: 일반적인 화질 설정 (낮을수록 고화질)\n",
        "    # -preset veryfast: 속도 우선 인코딩\n",
        "    # -an: 오디오 제거 (학습에 필요 없음, 용량 절감)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-y', '-i', str(src_path),\n",
        "        '-vf', f'scale={size[0]}:{size[1]}',\n",
        "        '-c:v', 'libx264', '-crf', '18', '-preset', 'medium',\n",
        "        '-an', str(dst_path)\n",
        "    ]\n",
        "\n",
        "    # 실행 (로그는 숨김)\n",
        "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "def preprocess_celebdf(src_root, dst_root, size=(384, 384), max_workers=8):\n",
        "    src_root = Path(src_root)\n",
        "    dst_root = Path(dst_root)\n",
        "\n",
        "    if dst_root.exists():\n",
        "        print(f\"Target directory {dst_root} already exists. Skipping preprocessing.\")\n",
        "        return\n",
        "\n",
        "    # 모든 mp4 파일 찾기\n",
        "    video_files = list(src_root.glob(\"*/*.mp4\"))\n",
        "    tasks = []\n",
        "\n",
        "    for src_path in video_files:\n",
        "        rel_path = src_path.relative_to(src_root)\n",
        "        dst_path = dst_root / rel_path\n",
        "        if not dst_path.exists():\n",
        "            tasks.append((src_path, dst_path, size))\n",
        "\n",
        "    print(f\"Starting preprocessing: {len(tasks)} videos...\")\n",
        "\n",
        "    # 멀티프로세싱으로 병렬 처리\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(executor.map(resize_single_video, tasks), total=len(tasks)))\n",
        "    copy2(src_root / \"List_of_testing_videos.txt\", dst_root / \"List_of_testing_videos.txt\")\n",
        "\n",
        "    print(\"Preprocessing completed.\")"
      ],
      "metadata": {
        "id": "_a_Hd4cVNV2H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CelebDFDataModule(L.LightningDataModule):\n",
        "    def __init__(self, length=32, batch_size=32, num_workers=8):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.preprocessed_path = \"/content/preprocessed\"\n",
        "\n",
        "    def prepare_data(self):\n",
        "        path = kagglehub.dataset_download(\"reubensuju/celeb-df-v2\")\n",
        "        preprocess_celebdf(path, self.preprocessed_path, max_workers=12)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        path = self.preprocessed_path\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            train_full = CelebDFDataset(is_test=False, path_str=path)\n",
        "            self.celebdf_train, self.celebdf_val = random_split(train_full, [0.9, 0.1])\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.celebdf_test = CelebDFDataset(is_test=True, path_str=path)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.celebdf_train, num_workers=self.num_workers, batch_size=self.batch_size, pin_memory=True, persistent_workers=True)\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.celebdf_val, num_workers=self.num_workers, batch_size=self.batch_size, pin_memory=True, persistent_workers=True)\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.celebdf_test, num_workers=self.num_workers, batch_size=self.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Ca-lGJcDUxBV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "wandb_key = userdata.get('wandb_api')\n",
        "wandb.login(key=wandb_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb8kC4NdXpMX",
        "outputId": "b0d0ee4f-7668-4ef4-c9ea-56dabd6dc846"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhnam0502\u001b[0m (\u001b[33mdhnam0502-likelion\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 20\n",
        "DECAY_STEPS = (1000 * 32) // BATCH_SIZE\n",
        "\n",
        "datamodule = CelebDFDataModule(batch_size=BATCH_SIZE)\n",
        "lit_classifier = LitUNITEClassifier(decay_steps=DECAY_STEPS)\n",
        "# lit_classifier = torch.compile(lit_classifier)"
      ],
      "metadata": {
        "id": "TvJmnC34YCFF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datamodule.prepare_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H637pv3OVro",
        "outputId": "e7fa4283-2cf8-4e95-d24c-06481ddc3a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n",
            "Starting preprocessing: 6529 videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 30/6529 [00:29<1:46:27,  1.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from lightning.pytorch.callbacks.lr_monitor import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from lightning.pytorch.callbacks import TQDMProgressBar\n",
        "\n",
        "\n",
        "wandb_logger = WandbLogger(project=\"UNITE_deepfake_classification\", name=\"baseline_profile\", log_model=True)\n",
        "\n",
        "ckpt = ModelCheckpoint(monitor=\"val/acc\", mode=\"max\", save_last=True)\n",
        "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "trainer =  L.Trainer(\n",
        "    # max_epochs=25,\n",
        "    max_steps=10,\n",
        "    profiler='simple',\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[ckpt, lr_monitor, TQDMProgressBar()],\n",
        "    precision='bf16-mixed'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHRx1khcaAoV",
        "outputId": "fac3bc56-fdd7-4ce3-952d-ea6502b79890"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "rkwkGvjlbUN6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(lit_classifier, datamodule=datamodule)"
      ],
      "metadata": {
        "id": "pNqNPyd7bQsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8kIxmiE6Lbbd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
