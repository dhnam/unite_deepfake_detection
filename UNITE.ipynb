{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a1ce30fc0d74854aecf0ef933870c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b8b899a21f44366a9fc94663845073f",
              "IPY_MODEL_83c369cd3c19427aa1de54d39ab140ac",
              "IPY_MODEL_d76d0e2ad0194595bd472d7bab2ed5de"
            ],
            "layout": "IPY_MODEL_b338e2597dfb48c6ae92f2061ab89ba7"
          }
        },
        "9b8b899a21f44366a9fc94663845073f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d2dbc5942544f128c4c29071043c481",
            "placeholder": "​",
            "style": "IPY_MODEL_655a7976f26141b183e63f09a02154b8",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "83c369cd3c19427aa1de54d39ab140ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3407f8b346af483fb315dd6141d3062c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80e788395f7e4776b94c4c928329f80d",
            "value": 0
          }
        },
        "d76d0e2ad0194595bd472d7bab2ed5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f0258a58f44a0287c09ba855a6f33b",
            "placeholder": "​",
            "style": "IPY_MODEL_5aec7f874fbe419cad00a7c77352560b",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "b338e2597dfb48c6ae92f2061ab89ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "3d2dbc5942544f128c4c29071043c481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "655a7976f26141b183e63f09a02154b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3407f8b346af483fb315dd6141d3062c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e788395f7e4776b94c4c928329f80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7f0258a58f44a0287c09ba855a6f33b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aec7f874fbe419cad00a7c77352560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋은 우선적으로는 CelebDFV2를 사용하고, 이후에 GTA-V 데이터를 합쳐보도록 하자."
      ],
      "metadata": {
        "id": "LbmeSoy6C9Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이걸 어디부터 구현해야 할까...\n",
        "\n",
        "일단 SigLIP을 불러오는 것부터 시작하자. Quantize를 해줘서 최대한 부담을 줄여주자."
      ],
      "metadata": {
        "id": "YpBO_bgZCCur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --system lightning\n",
        "!pip install -q lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvXKooN5CCOG",
        "outputId": "2caab3e1-61c5-4bba-9a08-153468621528"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 102ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --system bitsandbytes\n",
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5NX_61lI4WV",
        "outputId": "b1ba186e-d40d-4cf2-b3bb-8ae413337e6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CesM4HbVBZWY"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as L\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "embed_size = 768\n",
        "\n",
        "frame_token = 576"
      ],
      "metadata": {
        "id": "G1LPodHaYK4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHpA0CA-MaC_",
        "outputId": "86ad4985-09c3-4c93-e152-8ae990927634"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "솔직히, 왠만하면 원본 모델을 따라가고 싶지만... 일단 인코더를 더 경량으로 바꾼다.\n",
        "\n",
        "나중에 성능 안 나오면 탓할 것 중 인코더가 늘었다."
      ],
      "metadata": {
        "id": "RZwnOwZsXtZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "cTmv1qBoXnHs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "        assert self.head_dim * num_heads == embed_size, \"embed_size must be divisible by num_heads\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.k_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.v_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
        "        self.dropout_p = dropout\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x, return_head_contrib=False):\n",
        "        # x: (batch_size, seq_len, embed_size)\n",
        "        batch_size, seq_len, embed_size = x.shape\n",
        "\n",
        "        # Project queries, keys, values\n",
        "        q = self.q_proj(x) # (batch_size, seq_len, embed_size)\n",
        "        k = self.k_proj(x) # (batch_size, seq_len, embed_size)\n",
        "        v = self.v_proj(x) # (batch_size, seq_len, embed_size)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, seq_len, head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, seq_len, head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Apply scaled dot product attention\n",
        "        # dropout_p는 훈련 중일 때만 적용\n",
        "        attn_output_raw = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=self.dropout_p if self.training else 0.0,\n",
        "            is_causal=False\n",
        "        )\n",
        "        # attn_output: (b, num_heads, seq_len, head_dim)\n",
        "\n",
        "\n",
        "        head_contrib = None\n",
        "        if return_head_contrib:\n",
        "            # out_proj_weight: [embed_size, embed_size] -> [num_heads, head_dim, embed_size]\n",
        "            out_proj_weight = self.out_proj.weight.t().view(self.num_heads, self.head_dim, embed_size)\n",
        "            # b: batch size, h: num_head, l: seq_len, k: head_dim, d: embed_dim\n",
        "            head_contrib = torch.einsum(\"bhlk, hkd -> bhld\", attn_output_raw, out_proj_weight)\n",
        "            # -> result: [batch, num_head, seq_len, embed_dim] (-> [batch, num_head, seq_len, embed_dim])\n",
        "            # Have to remove class token, and pool that with frame dimension\n",
        "\n",
        "        # Concatenate heads and apply final linear projection\n",
        "        attn_output = attn_output_raw.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_size)\n",
        "        attn_output = self.out_proj(attn_output) # (batch_size, seq_len, embed_size)\n",
        "\n",
        "\n",
        "        x = self.ln1(x + attn_output) # Residual connection + LayerNorm\n",
        "        x = self.ln2(x + self.mlp(x))\n",
        "\n",
        "        if return_head_contrib:\n",
        "            return x, head_contrib\n",
        "        return x"
      ],
      "metadata": {
        "id": "0p36S4LXYIzn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float=0.1, max_len: int=32): # max_len을 32로 설정\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        # d_model에 맞춰 div_term 계산\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # [max_len, d_model] 형상으로 생성\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # [1, max_len, d_model]로 변경하여 Batch First 대응\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [batch_size, frame * seq_len, d_model]\n",
        "        \"\"\"\n",
        "        b, f, t, d = x.shape\n",
        "        # [1, max_len, 1, d_model]로 -> frame과 d_model에만 적용한다\n",
        "        # seq_len의 공간적 위치 정보는 이미 인코더에서 처리해주었다.\n",
        "        curr_pe = self.pe[:, :f, :].unsqueeze(2)\n",
        "        # 입력된 x의 길이만큼만 PE를 더함\n",
        "        x = x + curr_pe\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "IA7_g3dQb3rI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "class GPUSigLIPProcessor:\n",
        "    def __init__(self, processor):\n",
        "        config = processor.image_processor\n",
        "\n",
        "        # 1. 리사이즈 설정: Bilinear + Antialias=True가 핵심\n",
        "        # Fast 프로세서가 텐서를 처리할 때 사용하는 로직과 일치시킵니다.\n",
        "        self.resize = v2.Resize(\n",
        "            size=(config.size['height'], config.size['width']),\n",
        "            interpolation=v2.InterpolationMode.BILINEAR, # resample=2\n",
        "            antialias=True # 오차를 줄이는 가장 중요한 설정\n",
        "        )\n",
        "\n",
        "        # 2. 정규화 설정\n",
        "        # (x - 0.5) / 0.5 연산\n",
        "        self.mean = torch.tensor(config.image_mean).view(1, 3, 1, 1)\n",
        "        self.std = torch.tensor(config.image_std).view(1, 3, 1, 1)\n",
        "        self.rescale_factor = config.rescale_factor\n",
        "\n",
        "    def __call__(self, video_tensor):\n",
        "        \"\"\"\n",
        "        video_tensor: (B, 3, T, H, W), uint8, GPU\n",
        "        \"\"\"\n",
        "        b, c, t, h, w = video_tensor.shape\n",
        "        device = video_tensor.device\n",
        "\n",
        "        # 차원 변경 (B*T, C, H, W)\n",
        "        x = video_tensor.permute(0, 2, 1, 3, 4)\n",
        "        x = x.flatten(0, 1)\n",
        "\n",
        "        # [Step 1] Resize (uint8 상태에서 수행하거나 float32에서 수행)\n",
        "        # torchvision v2는 uint8 입력을 받아 내부적으로 고정밀 연산을 수행합니다.\n",
        "        x = self.resize(x)\n",
        "\n",
        "        # [Step 2] Float32 변환 및 Rescale (0~255 -> 0~1)\n",
        "        x = x.to(torch.float32) * self.rescale_factor\n",
        "\n",
        "        # [Step 3] Normalize (x - 0.5) / 0.5\n",
        "        # mean, std를 캐싱하여 속도 최적화\n",
        "        self.mean = self.mean.to(device)\n",
        "        self.std = self.std.to(device)\n",
        "        x = (x - self.mean) / self.std\n",
        "\n",
        "        # [Step 4] 최종 모델 입력형태인 float16으로 반환\n",
        "        return x.to(torch.float16)"
      ],
      "metadata": {
        "id": "Fx_U3LxUoMqM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNITE(nn.Module):\n",
        "    def __init__(self, num_channel=3, num_cls=2, num_heads=12, max_len=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        model_id = \"google/siglip2-base-patch16-384\"\n",
        "        self.vis_encoder = AutoModel.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            dtype=torch.bfloat16\n",
        "        )\n",
        "        self.embed_size = self.vis_encoder.config.vision_config.hidden_size\n",
        "        processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
        "        self.processor = GPUSigLIPProcessor(processor)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        for para in self.vis_encoder.parameters():\n",
        "            para.requires_grad = False\n",
        "        self.vis_encoder.eval()\n",
        "\n",
        "        self.class_token = nn.Parameter(torch.randn((self.embed_size,)), requires_grad=True)\n",
        "\n",
        "        self.pos_embedding = PositionalEncoding(self.embed_size)\n",
        "        self.first_encoder = ViTEncoder(self.embed_size, num_heads, dropout)\n",
        "        self.encoders = nn.ModuleList([ViTEncoder(self.embed_size, num_heads, dropout) for _ in range(3)])\n",
        "        self.mlp_head = nn.Linear(self.embed_size, num_cls)\n",
        "\n",
        "\n",
        "    def forward(self, x, return_ad_param=False):\n",
        "        self.vis_encoder.eval()\n",
        "\n",
        "        # Input: [batch, c, frame, h, w]\n",
        "        b, _, f, *_ = x.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # -> Preprocessing [batch * frame, c, h, w]\n",
        "            x = self.processor(x)\n",
        "            # -> Visual encoding [batch * frame, token/frame(576), dim/token (embed_size)]\n",
        "            x = self.vis_encoder.vision_model(pixel_values=x).last_hidden_state\n",
        "        # -> [batch, frame, token/frame, dim/token]\n",
        "        x = x.reshape(b, f, -1, self.embed_size)\n",
        "        x = self.pos_embedding(x)\n",
        "        train_in = x # xi\n",
        "\n",
        "        _, _, t, d = x.shape\n",
        "        # Reshape for transformer\n",
        "        # -> [batch, total token, dim/token]\n",
        "        x = x.reshape(b, f*t, d)\n",
        "        # Add class token\n",
        "        cls_token = self.class_token.view(1, 1, -1).expand(b, -1, -1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        P = None\n",
        "        if return_ad_param:\n",
        "            x, head_contrib = self.first_encoder(x, return_head_contrib=True)\n",
        "            # head_contrib: [batch, head, total token + class token, dim/token]\n",
        "            # -> [batch, head, total token, dim/token]\n",
        "            head_contrib = head_contrib[:, :, 1:, :]\n",
        "            # -> [batch, head, frame, token/frame, dim/token]\n",
        "            head_contrib = head_contrib.view(b, self.num_heads, f, t, d)\n",
        "            # -> mean pooling [batch, head, token/frame, dim/token]\n",
        "            head_contrib = head_contrib.mean(dim=2) # A\n",
        "\n",
        "            P = torch.einsum(\"bftd, bhtd -> bhf\", train_in, head_contrib)\n",
        "        else:\n",
        "            x = self.first_encoder(x)\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "\n",
        "        # Get only cls_token\n",
        "        x = x[:, 0, :]\n",
        "        x = x.view(b, -1)\n",
        "        x = self.mlp_head(x)\n",
        "        if return_ad_param:\n",
        "            return x, P\n",
        "        return x"
      ],
      "metadata": {
        "id": "Dl5XX6bXbf46"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ADLoss(nn.Module):\n",
        "    def __init__(self, num_cls=2, num_heads=12, max_len=32, delta_within=(0.01, -2.0), delta_between=0.5, eta=0.05):\n",
        "        super().__init__()\n",
        "        # C shape: [num_classes, num_heads, max_len]\n",
        "        # 논문 식(3)에 따라 센터를 각 클래스별로 유지해야 함\n",
        "        C = torch.zeros(num_cls, num_heads, max_len)\n",
        "        self.register_buffer('C', C)\n",
        "\n",
        "        self.num_cls = num_cls\n",
        "        self.delta_within = torch.tensor(delta_within) # [0.01, -2.0] (True, Fake)\n",
        "        self.delta_between = delta_between # 0.5\n",
        "        self.eta = eta\n",
        "\n",
        "    def forward(self, P, labels, log_detail=False):\n",
        "        \"\"\"\n",
        "        P: [batch, num_heads, max_len] (Pooled Features)\n",
        "        labels: [batch] (Class indices)\n",
        "        \"\"\"\n",
        "        device = P.device\n",
        "        self.delta_within = self.delta_within.to(device)\n",
        "\n",
        "        P_norm = F.normalize(P.view(P.size(0), -1), p=2, dim=1).view_as(P)\n",
        "        C_norm = F.normalize(self.C.view(self.num_cls, -1), p=2, dim=1).view_as(self.C)\n",
        "\n",
        "        # --- 1. 센터 업데이트 (식 3) ---\n",
        "        # 배치의 각 클래스별 평균을 구해서 업데이트\n",
        "        for c in range(self.num_cls):\n",
        "            mask = (labels == c)\n",
        "            if mask.any():\n",
        "                # 해당 클래스의 이번 배치 평균\n",
        "                batch_class_mean = P_norm[mask].mean(dim=0) # [num_heads, max_len]\n",
        "                # print(f\"{self.C[c].shape, P.shape, P[mask].shape, batch_class_mean.shape=}\")\n",
        "                # 이동 평균 업데이트\n",
        "                with torch.no_grad():\n",
        "                    self.C[c] = (1 - self.eta) * self.C[c] + self.eta * batch_class_mean.detach()\n",
        "\n",
        "        # --- 2. Within-class Loss (식 4) ---\n",
        "        # 각 샘플과 자기 클래스 센터 사이의 거리\n",
        "        # P: [B, H, F], self.C[labels]: [B, H, F]\n",
        "        diff_within = P_norm - C_norm[labels]\n",
        "        # L2 Norm 계산 (헤드와 프레임 차원에 대해)\n",
        "        dist_within = torch.norm(diff_within, p=2, dim=(1, 2))\n",
        "\n",
        "        # 각 샘플별 delta 적용\n",
        "        loss_within = torch.relu(dist_within - self.delta_within[labels]).mean()\n",
        "\n",
        "        # --- 3. Between-class Loss (식 5: 서로 다른 쌍에 대해 전부) ---\n",
        "        # 클래스 센터들 간의 모든 쌍 거리 (Pairwise Distance) 계산\n",
        "        # self.C: [num_classes, H*F]로 펼쳐서 계산하면 편리함\n",
        "        C_flat = C_norm.view(self.num_cls, -1)\n",
        "\n",
        "        # 모든 클래스 쌍 간의 차이 계산: [num_classes, num_classes, H*F]\n",
        "        # Broadcasting 활용: (N, 1, D) - (1, N, D) -> (N, N, D)\n",
        "        diff_between = C_flat.unsqueeze(1) - C_flat.unsqueeze(0)\n",
        "\n",
        "        # 모든 쌍의 거리 행렬: [num_classes, num_classes]\n",
        "        dist_matrix = torch.norm(diff_between, p=2, dim=2)\n",
        "\n",
        "        # k != l 인 조건 (서로 다른 쌍) 추출을 위한 마스크\n",
        "        # torch.triu를 써서 중복 계산(k,l과 l,k)을 피하고 자기 자신(k=l)도 제외함\n",
        "        mask_between = torch.triu(torch.ones(self.num_cls, self.num_cls, device=device), diagonal=1).bool()\n",
        "\n",
        "        # 서로 다른 클래스 쌍의 거리들만 추출\n",
        "        different_pairs_dist = dist_matrix[mask_between]\n",
        "\n",
        "        # 식 (5) 적용: max(delta - dist, 0)\n",
        "        loss_between = torch.relu(self.delta_between - different_pairs_dist).sum()\n",
        "        if log_detail:\n",
        "            return loss_within + loss_between, loss_within, loss_between\n",
        "\n",
        "        return loss_within + loss_between"
      ],
      "metadata": {
        "id": "OflIU8asnbGT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import Accuracy, AveragePrecision, Precision, Recall"
      ],
      "metadata": {
        "id": "w_3YGmU93VT8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LitUNITEClassifier(L.LightningModule):\n",
        "    def __init__(\n",
        "            self, num_cls=2, num_heads=12, max_len=32, dropout=0.1,\n",
        "            delta_within=(0.01, -2.0), delta_between=0.5, eta=0.05,\n",
        "            lambda_1=0.5, lambda_2=0.5, lr=1e-4, decay_steps=1000,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = UNITE(\n",
        "            num_cls=num_cls,\n",
        "            num_heads=num_heads,\n",
        "            max_len=max_len,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.ad_loss = ADLoss(\n",
        "            num_cls=num_cls,\n",
        "            num_heads=num_heads,\n",
        "            max_len=max_len,\n",
        "            delta_within=delta_within,\n",
        "            delta_between=delta_between,\n",
        "            eta=eta,\n",
        "        )\n",
        "        self.lambda_1 = lambda_1\n",
        "        self.lambda_2 = lambda_2\n",
        "\n",
        "        self.lr = lr\n",
        "        self.decay_steps = decay_steps # Set this in respect to batch size; original batch size was 32\n",
        "\n",
        "        self.acc = Accuracy(task='multiclass', num_classes=num_cls)\n",
        "        self.ap = AveragePrecision(task='multiclass', num_classes=num_cls)\n",
        "        self.precision = Precision(task='multiclass', num_classes=num_cls)\n",
        "        self.recall = Recall(task='multiclass', num_classes=num_cls)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit, P = self.model(x, return_ad_param=True)\n",
        "        loss_ad, within, between = self.ad_loss(P, y, log_detail=True)\n",
        "        loss_ce = self.ce_loss(logit, y)\n",
        "        loss = loss_ce * self.lambda_1 + loss_ad * self.lambda_2\n",
        "        self.log(\"train/loss_ad\", loss_ad, logger=True)\n",
        "        self.log(\"train/loss_ad/loss_within\", within, logger=True)\n",
        "        self.log(\"train/loss_ad/loss_between\", between, logger=True)\n",
        "        self.log(\"train/loss_ce\", loss_ce, logger=True)\n",
        "        self.log(\"train/loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit, P = self.model(x, return_ad_param=True)\n",
        "        loss_ad = self.ad_loss(P, y)\n",
        "        loss_ce = self.ce_loss(logit, y)\n",
        "        loss = loss_ce * self.lambda_1 + loss_ad * self.lambda_2\n",
        "        self.log(\"val/loss_ad\", loss_ad, logger=True)\n",
        "        self.log(\"val/loss_ce\", loss_ce, logger=True)\n",
        "        self.log(\"val/loss\", loss, prog_bar=True, logger=True)\n",
        "\n",
        "        self.acc(logit, y)\n",
        "        self.ap(logit, y)\n",
        "        self.precision(logit, y)\n",
        "        self.recall(logit, y)\n",
        "        self.log(\"val/acc\", self.acc, logger=True)\n",
        "        self.log(\"val/ap\", self.ap, logger=True)\n",
        "        self.log(\"val/precision\", self.precision, logger=True)\n",
        "        self.log(\"val/recall\", self.recall, logger=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logit = self.model(x)\n",
        "\n",
        "        self.acc(logit, y)\n",
        "        self.ap(logit, y)\n",
        "        self.precision(logit, y)\n",
        "        self.recall(logit, y)\n",
        "        self.log(\"test/acc\", self.acc, logger=True)\n",
        "        self.log(\"test/ap\", self.ap, logger=True)\n",
        "        self.log(\"test/precision\", self.precision, logger=True)\n",
        "        self.log(\"test/recall\", self.recall, logger=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optim, self.decay_steps, gamma=0.5)\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optim,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\",\n",
        "            },\n",
        "        }"
      ],
      "metadata": {
        "id": "Mwpa-9_OuUpJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "class CelebDFDataset(Dataset):\n",
        "    def __init__(self, is_test: bool, path_str:str, length=32, size=(384, 384), transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            is_test (bool): True면 테스트 셋, False면 트레인 셋 로드\n",
        "            length (int): 시퀀스 길이 (기본 32)\n",
        "            transform: 이미지 전처리 (Optional)\n",
        "        \"\"\"\n",
        "\n",
        "        self.path = Path(path_str)\n",
        "        self.is_test = is_test\n",
        "        self.length = length\n",
        "        self.size = size\n",
        "        self.transform = transform\n",
        "\n",
        "        # 모든 mp4 파일 검색\n",
        "        self.files = list(self.path.glob(\"*/*.mp4\"))\n",
        "\n",
        "        # 테스트 비디오 리스트 로드\n",
        "        # 파일 형식: [1|0] [path] (예: 1 YouTube-real/00170.mp4)\n",
        "        txt_path = self.path / \"List_of_testing_videos.txt\"\n",
        "        test_df = pd.read_csv(txt_path, sep=\" \", header=None, names=[\"label\", \"path\"])\n",
        "\n",
        "        # 비교를 위해 테스트 파일 경로들을 Set으로 변환 (검색 속도 향상)\n",
        "        # Windows/Linux 경로 구분자 통일을 위해 '/'로 replace 처리\n",
        "        test_files_set = set(test_df[\"path\"].apply(lambda x: x.replace(\"\\\\\", \"/\")).values)\n",
        "\n",
        "        self.samples = [] # (video_path, chunk_index, label) 튜플을 저장할 리스트\n",
        "\n",
        "        print(f\"Processing metadata for {'Test' if is_test else 'Train'} set...\")\n",
        "\n",
        "        for next_file in self.files:\n",
        "            # 데이터셋 루트 기준 상대 경로 (예: YouTube-real/00170.mp4)\n",
        "            rel_path = str(next_file.relative_to(self.path)).replace(\"\\\\\", \"/\")\n",
        "\n",
        "            # 현재 파일이 테스트 리스트에 있는지 확인\n",
        "            is_in_test_list = rel_path in test_files_set\n",
        "\n",
        "            # 요청한 Split(Train/Test)과 맞지 않으면 스킵\n",
        "            if self.is_test != is_in_test_list:\n",
        "                continue\n",
        "\n",
        "            # 레이블 결정 (0: Real, 1: Fake)\n",
        "            # 폴더명 기반 판단\n",
        "            if \"YouTube-real\" in rel_path or \"Celeb-real\" in rel_path:\n",
        "                label = 0\n",
        "            elif \"Celeb-synthesis\" in rel_path:\n",
        "                label = 1\n",
        "            else:\n",
        "                continue # 알 수 없는 폴더는 제외\n",
        "\n",
        "            cap = cv2.VideoCapture(str(next_file))\n",
        "            if cap.isOpened():\n",
        "                frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "                cap.release()\n",
        "\n",
        "                if frame_cnt <= 0: continue\n",
        "\n",
        "                # 영상 하나를 여러 데이터(Chunk)로 쪼개기\n",
        "                # Stride=2 (하나 걸러 하나), Length=32\n",
        "                # 데이터 하나당 필요한 원본 프레임 구간 = 약 64 프레임\n",
        "                # 전체 프레임에서 Stride 2로 뽑았을 때 나오는 유효 프레임 수\n",
        "                effective_frames = math.ceil(frame_cnt / 2)\n",
        "\n",
        "                # 영상 하나에서 나오는 데이터 개수 (올림 처리)\n",
        "                num_chunks = math.ceil(effective_frames / self.length)\n",
        "\n",
        "                for i in range(num_chunks):\n",
        "                    self.samples.append({\n",
        "                        \"video_path\": str(next_file),\n",
        "                        \"chunk_idx\": i,\n",
        "                        \"label\": label\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"Cannot open video: {next_file}\")\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} samples from {len(self.files)} files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        meta = self.samples[idx]\n",
        "        video_path = meta[\"video_path\"]\n",
        "        chunk_idx = meta[\"chunk_idx\"]\n",
        "        label = meta[\"label\"]\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # 시작 프레임 계산 (Chunk 인덱스 * 시퀀스 길이 * Stride 2)\n",
        "        start_frame = chunk_idx * self.length * 2\n",
        "\n",
        "        frames = []\n",
        "\n",
        "        # 시작 위치로 이동\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        # 최대 32개의 유효 프레임을 모을 때까지 반복\n",
        "        # Stride 2를 구현하기 위해 읽으면서 짝수 번째만 저장하거나, 2프레임씩 건너뜀\n",
        "        # 여기서는 안전하게 프레임을 순차적으로 읽으며 인덱스를 체크합니다.\n",
        "\n",
        "        # 읽어야 할 최대 범위 (32개를 모으기 위해 최대 64프레임 탐색)\n",
        "        for i in range(self.length * 2):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break # 영상 끝 도달\n",
        "\n",
        "            # 짝수 번째(0, 2, 4...) 프레임만 수집 (하나 걸러 하나)\n",
        "            if i % 2 == 0:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                # 필요하다면 여기서 cv2.resize 등을 수행\n",
        "                frame = cv2.resize(frame, self.size)\n",
        "                frames.append(frame)\n",
        "\n",
        "            if len(frames) == self.length:\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # 패딩 로직: 32개가 안 되면 마지막 프레임으로 채움\n",
        "        if len(frames) < self.length:\n",
        "            if len(frames) == 0:\n",
        "                # 매우 드문 경우 (파일은 열리는데 프레임이 없는 경우 등) -> 0으로 채움\n",
        "                # 일반적으로는 발생하지 않아야 함\n",
        "                h, w = 224, 224 # 기본 크기 가정\n",
        "                frames = [np.zeros((h, w, 3), dtype=np.uint8) for _ in range(self.length)]\n",
        "            else:\n",
        "                last_frame = frames[-1]\n",
        "                while len(frames) < self.length:\n",
        "                    frames.append(last_frame.copy())\n",
        "\n",
        "        # Numpy array 변환: (T, H, W, C) -> (32, H, W, 3)\n",
        "        frames_np = np.array(frames)\n",
        "\n",
        "        if self.transform:\n",
        "            # Transform이 있다면 적용 (보통 이미지 단위로 적용)\n",
        "            # Video transform 라이브러리를 쓴다면 그대로 넘겨야 함\n",
        "            # 여기서는 간단히 Torch Tensor 변환 예시\n",
        "            pass\n",
        "\n",
        "        # To Tensor: (T, H, W, C) -> (C, T, H, W) 형태로 변환 (PyTorch Video 모델 표준)\n",
        "        # 0~255 값을 0~1로 정규화는 추후에 할 예정이므로 삭제\n",
        "        frames_tensor = torch.from_numpy(frames_np).permute(3, 0, 1, 2)\n",
        "\n",
        "        return frames_tensor, torch.tensor(label)"
      ],
      "metadata": {
        "id": "mCig1aG-MBvI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CelebDFDataModule(L.LightningDataModule):\n",
        "    def __init__(self, length=32, batch_size=32, num_workers=8):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def prepare_data(self):\n",
        "        kagglehub.dataset_download(\"reubensuju/celeb-df-v2\")\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        path = kagglehub.dataset_download(\"reubensuju/celeb-df-v2\")\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            train_full = CelebDFDataset(is_test=False, path_str=path)\n",
        "            self.celebdf_train, self.celebdf_val = random_split(train_full, [0.9, 0.1])\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.celebdf_test = CelebDFDataset(is_test=True, path_str=path)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.celebdf_train, num_workers=self.num_workers, batch_size=self.batch_size, pin_memory=True)\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.celebdf_val, num_workers=self.num_workers, batch_size=self.batch_size, pin_memory=True)\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.celebdf_test, num_workers=self.num_workers, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "Ca-lGJcDUxBV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "wandb_key = userdata.get('wandb_api')\n",
        "wandb.login(key=wandb_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb8kC4NdXpMX",
        "outputId": "c42af048-f185-442e-eade-c5f58b12ac02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhnam0502\u001b[0m (\u001b[33mdhnam0502-likelion\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 20\n",
        "DECAY_STEPS = (1000 * 32) // BATCH_SIZE\n",
        "\n",
        "datamodule = CelebDFDataModule(batch_size=BATCH_SIZE)\n",
        "lit_classifier = LitUNITEClassifier(decay_steps=DECAY_STEPS)\n",
        "lit_classifier = torch.compile(lit_classifier)"
      ],
      "metadata": {
        "id": "TvJmnC34YCFF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from lightning.pytorch.callbacks.lr_monitor import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from lightning.pytorch.callbacks import TQDMProgressBar\n",
        "\n",
        "\n",
        "wandb_logger = WandbLogger(project=\"UNITE_deepfake_classification\", name=\"baseline\", log_model=True)\n",
        "\n",
        "ckpt = ModelCheckpoint(monitor=\"val/acc\", mode=\"max\", save_last=True)\n",
        "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "trainer =  L.Trainer(\n",
        "    max_epochs=25,\n",
        "    # profiler='simple',\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[ckpt, lr_monitor, TQDMProgressBar()],\n",
        "    precision='bf16-mixed'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHRx1khcaAoV",
        "outputId": "36f9cdad-155c-453f-dfa5-4f64a0c64de1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "rkwkGvjlbUN6",
        "outputId": "d7682aa3-17d9-4152-fdf5-fcf8663a8703"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁</td></tr><tr><td>lr-AdamW</td><td>▁</td></tr><tr><td>train/loss</td><td>▁█▄▄</td></tr><tr><td>train/loss_ad</td><td>█▁▆▄</td></tr><tr><td>train/loss_ad/loss_between</td><td>▁▇██</td></tr><tr><td>train/loss_ad/loss_within</td><td>█▁▆▄</td></tr><tr><td>train/loss_ce</td><td>▁█▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▃▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>lr-AdamW</td><td>0.0001</td></tr><tr><td>train/loss</td><td>1.31554</td></tr><tr><td>train/loss_ad</td><td>2.3158</td></tr><tr><td>train/loss_ad/loss_between</td><td>0.49422</td></tr><tr><td>train/loss_ad/loss_within</td><td>1.82158</td></tr><tr><td>train/loss_ce</td><td>0.31529</td></tr><tr><td>trainer/global_step</td><td>199</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">baseline</strong> at: <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/azrr8xke' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/azrr8xke</a><br> View project at: <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>wandb/run-20260130_081518-azrr8xke/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(lit_classifier, datamodule=datamodule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591,
          "referenced_widgets": [
            "1a1ce30fc0d74854aecf0ef933870c99",
            "9b8b899a21f44366a9fc94663845073f",
            "83c369cd3c19427aa1de54d39ab140ac",
            "d76d0e2ad0194595bd472d7bab2ed5de",
            "b338e2597dfb48c6ae92f2061ab89ba7",
            "3d2dbc5942544f128c4c29071043c481",
            "655a7976f26141b183e63f09a02154b8",
            "3407f8b346af483fb315dd6141d3062c",
            "80e788395f7e4776b94c4c928329f80d",
            "f7f0258a58f44a0287c09ba855a6f33b",
            "5aec7f874fbe419cad00a7c77352560b"
          ]
        },
        "id": "pNqNPyd7bQsH",
        "outputId": "2a62116d-6e3a-441d-efa0-667585250491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>wandb/run-20260130_083100-pe6gb4bj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/pe6gb4bj' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/pe6gb4bj' target=\"_blank\">https://wandb.ai/dhnam0502-likelion/UNITE_deepfake_classification/runs/pe6gb4bj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'celeb-df-v2' dataset.\n",
            "Processing metadata for Train set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 38569 samples from 6529 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model     │ UNITE                      │  403 M │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ ce_loss   │ CrossEntropyLoss           │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ ad_loss   │ ADLoss                     │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ acc       │ MulticlassAccuracy         │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ ap        │ MulticlassAveragePrecision │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ precision │ MulticlassPrecision        │      0 │ train │     0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ recall    │ MulticlassRecall           │      0 │ train │     0 │\n",
              "└───┴───────────┴────────────────────────────┴────────┴───────┴───────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model     │ UNITE                      │  403 M │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ ce_loss   │ CrossEntropyLoss           │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ ad_loss   │ ADLoss                     │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ acc       │ MulticlassAccuracy         │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ ap        │ MulticlassAveragePrecision │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ precision │ MulticlassPrecision        │      0 │ train │     0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ recall    │ MulticlassRecall           │      0 │ train │     0 │\n",
              "└───┴───────────┴────────────────────────────┴────────┴───────┴───────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 28.4 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 375 M                                                                                        \n",
              "\u001b[1mTotal params\u001b[0m: 403 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1.6 K                                                                      \n",
              "\u001b[1mModules in train mode\u001b[0m: 63                                                                                          \n",
              "\u001b[1mModules in eval mode\u001b[0m: 312                                                                                          \n",
              "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 28.4 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 375 M                                                                                        \n",
              "<span style=\"font-weight: bold\">Total params</span>: 403 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1.6 K                                                                      \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 63                                                                                          \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 312                                                                                          \n",
              "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a1ce30fc0d74854aecf0ef933870c99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbdZ5wsreQrE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
